{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xNo2F6QIlafd",
    "outputId": "492da1c8-02be-4e2d-a5ef-fb649affae7a"
   },
   "outputs": [],
   "source": [
    "#!pip install chex\n",
    "#!pip install dm-haiku\n",
    "#!pip install optax\n",
    "#!pip install distrax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e622a29d"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "import distrax\n",
    "import chex\n",
    "import numpy as np\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0e695ca"
   },
   "outputs": [],
   "source": [
    "# Use a EBM formulation of likelihod vs a score formulation of likelihood\n",
    "ebm = True\n",
    "\n",
    "# Number of diffusion timesteps to train\n",
    "n_steps = 100\n",
    "data_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0b656d20"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21634c68"
   },
   "outputs": [],
   "source": [
    "# Define a simple MLP Diffusion Model\n",
    "class ResnetDiffusionModel(hk.Module):\n",
    "  \"\"\"Resnet score model.\n",
    "\n",
    "  Adds embedding for each scale after each linear layer.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               n_steps,\n",
    "               n_layers,\n",
    "               x_dim,\n",
    "               h_dim,\n",
    "               emb_dim,\n",
    "               widen=2,\n",
    "               emb_type='learned',\n",
    "               name=None):\n",
    "    assert emb_type in ('learned', 'sinusoidal')\n",
    "    super().__init__(name=name)\n",
    "    self._n_layers = n_layers\n",
    "    self._n_steps = n_steps\n",
    "    self._x_dim = x_dim\n",
    "    self._h_dim = h_dim\n",
    "    self._emb_dim = emb_dim\n",
    "    self._widen = widen\n",
    "    self._emb_type = emb_type\n",
    "\n",
    "  def __call__(self, x, t):\n",
    "\n",
    "    x = jnp.atleast_2d(x)\n",
    "    t = jnp.atleast_1d(t)\n",
    "\n",
    "    chex.assert_shape(x, (None, self._x_dim))\n",
    "    chex.assert_shape(t, (None,))\n",
    "    chex.assert_type([x, t], [jnp.float32, jnp.int64])\n",
    "\n",
    "    if self._emb_type == 'learned':\n",
    "      emb = hk.Embed(self._n_steps, self._emb_dim)(t)\n",
    "    else:\n",
    "      emb = timestep_embedding(t, self._emb_dim)\n",
    "\n",
    "    x = hk.Linear(self._h_dim)(x)\n",
    "\n",
    "    for _ in range(self._n_layers):\n",
    "      # get layers and embeddings\n",
    "      layer_h = hk.Linear(self._h_dim * self._widen)\n",
    "      layer_emb = hk.Linear(self._h_dim * self._widen)\n",
    "      layer_int = hk.Linear(self._h_dim * self._widen)\n",
    "      layer_out = hk.Linear(self._h_dim, w_init=jnp.zeros)\n",
    "\n",
    "      h = hk.LayerNorm(-1, True, True)(x)\n",
    "      h = jax.nn.swish(h)\n",
    "      h = layer_h(h)\n",
    "      h += layer_emb(emb)\n",
    "      h = jax.nn.swish(h)\n",
    "      h = layer_int(h)\n",
    "      h = jax.nn.swish(h)\n",
    "      h = layer_out(h)\n",
    "      x += h\n",
    "\n",
    "    x = hk.Linear(self._x_dim, w_init=jnp.zeros)(x)\n",
    "    chex.assert_shape(x, (None, self._x_dim))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "471ad970"
   },
   "outputs": [],
   "source": [
    "# Define a energy diffusion model (wrapper around a normal diffusion model)\n",
    "class EBMDiffusionModel(hk.Module):\n",
    "  \"\"\"EBM parameterization on top of score model.\n",
    "\n",
    "  Adds embedding for each scale after each linear layer.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, net, name=None):\n",
    "    super().__init__(name=name)\n",
    "    self.net = net\n",
    "\n",
    "  def neg_logp_unnorm(self, x, t):\n",
    "    score = self.net(x, t)\n",
    "    return ((score - x) ** 2).sum(-1)\n",
    "\n",
    "  def __call__(self, x, t):\n",
    "    neg_logp_unnorm = lambda _x: self.neg_logp_unnorm(_x, t).sum()\n",
    "    return hk.grad(neg_logp_unnorm)(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e034a9c0"
   },
   "outputs": [],
   "source": [
    "# Define how to multiply two different EBM distributions together\n",
    "class ProductEBMDiffusionModel(hk.Module):\n",
    "  \"\"\"EBM where we compose two distributions together.\n",
    "\n",
    "  Add the energy value together\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, net, net2, name=None):\n",
    "    super().__init__(name=name)\n",
    "    self.net = net\n",
    "    self.net2 = net2\n",
    "\n",
    "  def neg_logp_unnorm(self, x, t):\n",
    "    unorm_1 = self.net.neg_logp_unnorm(x, t)\n",
    "    unorm_2 = self.net2.neg_logp_unnorm(x, t)\n",
    "    return unorm_1 + unorm_2\n",
    "\n",
    "  def __call__(self, x, t):\n",
    "    score = (self.net(x, t) + self.net2(x, t))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41c7393a"
   },
   "outputs": [],
   "source": [
    "# Define how to add two different EBM distributions\n",
    "class MixtureEBMDiffusionModel(hk.Module):\n",
    "  \"\"\"EBM where we compose two distributions together.\n",
    "\n",
    "  Take the logsumexp of the energies\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, net, net2, name=None):\n",
    "    super().__init__(name=name)\n",
    "    self.net = net\n",
    "    self.net2 = net2\n",
    "\n",
    "  def neg_logp_unnorm(self, x, t):\n",
    "    unorm_1 = self.net.neg_logp_unnorm(x, t)\n",
    "    unorm_2 = self.net2.neg_logp_unnorm(x, t)\n",
    "    concat_energy = jnp.stack([unorm_1, unorm_2], axis=-1)\n",
    "    energy = -jax.scipy.special.logsumexp(-concat_energy*3.5, -1)\n",
    "\n",
    "    return energy\n",
    "\n",
    "  def __call__(self, x, t):\n",
    "    neg_logp_unnorm = lambda _x: self.neg_logp_unnorm(_x, t).sum()\n",
    "    return hk.grad(neg_logp_unnorm)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94671951"
   },
   "outputs": [],
   "source": [
    "class NegationEBMDiffusionModel(hk.Module):\n",
    "  \"\"\"EBM where we compose two distributions together.\n",
    "\n",
    "  Negate one distribution\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, net, net2, name=None):\n",
    "    super().__init__(name=name)\n",
    "    self.net = net\n",
    "    self.net2 = net2\n",
    "\n",
    "  def neg_logp_unnorm(self, x, t):\n",
    "    unorm_1 = self.net.neg_logp_unnorm(x, t)\n",
    "    unorm_2 = self.net2.neg_logp_unnorm(x, t)\n",
    "    return 1.3 * unorm_1 - 0.3 * unorm_2\n",
    "\n",
    "  def __call__(self, x, t):\n",
    "    neg_logp_unnorm = lambda _x: self.neg_logp_unnorm(_x, t).sum()\n",
    "    return hk.grad(neg_logp_unnorm)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebf6767b"
   },
   "outputs": [],
   "source": [
    "def extract(a: chex.Array,\n",
    "           t: chex.Array,\n",
    "           x_shape) -> chex.Array:\n",
    " \"\"\"Get coefficients at given timesteps and reshape to [batch_size, 1, ...].\"\"\"\n",
    " chex.assert_rank(t, 1)\n",
    " bs, = t.shape\n",
    " assert x_shape[0] == bs\n",
    " a = jax.device_put(a)\n",
    " out = a[t]\n",
    "\n",
    " assert out.shape[0] == bs\n",
    "\n",
    " return out.reshape([bs] + (len(x_shape) - 1) * [1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f568068a"
   },
   "outputs": [],
   "source": [
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    steps = timesteps + 1\n",
    "    x = np.linspace(0, timesteps, steps, dtype=np.float64)\n",
    "    alphas_cumprod = np.cos(((x / timesteps) + s) / (1 + s) * np.pi * 0.5)**2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return np.clip(betas, 0, 0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDnAuDl8vkWg"
   },
   "source": [
    "Below is the code for a simple diffusion trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ddadccf5"
   },
   "outputs": [],
   "source": [
    "# Simple diffusion model training\n",
    "class PortableDiffusionModel(hk.Module):\n",
    "  \"\"\"Basic Diffusion Model.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               dim,\n",
    "               n_steps,\n",
    "               net,\n",
    "               loss_type='simple',\n",
    "               mc_loss=True,\n",
    "               var_type='learned',\n",
    "               samples_per_step=1,\n",
    "               name=None):\n",
    "    super().__init__(name=name)\n",
    "    assert var_type in ('beta_forward', 'beta_reverse', 'learned')\n",
    "    self._var_type = var_type\n",
    "    self.net = net\n",
    "    self._n_steps = n_steps\n",
    "    self._dim = dim\n",
    "    self._loss_type = loss_type\n",
    "    self._mc_loss = mc_loss\n",
    "    self._samples_per_step = samples_per_step\n",
    "    self._betas = cosine_beta_schedule(n_steps)\n",
    "\n",
    "    self._alphas = 1. - self._betas\n",
    "    self._log_alphas = jnp.log(self._alphas)\n",
    "\n",
    "    alphas = 1. - self._betas\n",
    "\n",
    "    self._sqrt_alphas = jnp.array(jnp.sqrt(alphas), dtype=jnp.float32)\n",
    "    self._sqrt_recip_alphas = jnp.array(1. / jnp.sqrt(alphas), dtype=jnp.float32)\n",
    "\n",
    "    self._alphas_cumprod = jnp.cumprod(self._alphas, axis=0)\n",
    "    self._alphas_cumprod_prev = jnp.append(1., self._alphas_cumprod[:-1])\n",
    "    self._sqrt_alphas_cumprod = jnp.sqrt(self._alphas_cumprod)\n",
    "    self._sqrt_one_minus_alphas_cumprod = jnp.sqrt(1 - self._alphas_cumprod)\n",
    "    self._log_one_minus_alphas_cumprod = jnp.log(1 - self._alphas_cumprod)\n",
    "\n",
    "    self._sqrt_recip_alphas_cumprod = jax.lax.rsqrt(self._alphas_cumprod)\n",
    "    self._sqrt_recipm1_alphas_cumprod = jnp.sqrt(1 / self._alphas_cumprod - 1)\n",
    "    self._sqrt_recipm1_alphas_cumprod_custom = jnp.sqrt(1. / (1 - self._alphas_cumprod))\n",
    "\n",
    "    # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "    self._posterior_variance = self._betas * (\n",
    "        1. - self._alphas_cumprod_prev) / (1. - self._alphas_cumprod)\n",
    "\n",
    "    self._posterior_log_variance_clipped = jnp.log(\n",
    "        jnp.clip(self._posterior_variance, a_min=jnp.min(self._betas)))\n",
    "    self._posterior_mean_coef1 = self._betas * jnp.sqrt(\n",
    "        self._alphas_cumprod_prev) / (1 - self._alphas_cumprod)\n",
    "    self._posterior_mean_coef2 = (1 - self._alphas_cumprod_prev) * jnp.sqrt(\n",
    "        self._alphas) / (1 - self._alphas_cumprod)\n",
    "\n",
    "    self._out_logvar = hk.get_parameter('out_logvar',\n",
    "                                        shape=(n_steps,),\n",
    "                                        init=hk.initializers.Constant(\n",
    "                                            jnp.log(self._betas)))\n",
    "  def energy_scale(self, t):\n",
    "    return self._sqrt_recipm1_alphas_cumprod[t]\n",
    "\n",
    "  def data_scale(self, t):\n",
    "    return self._sqrt_recip_alphas_cumprod[t]\n",
    "\n",
    "  def forward(self, x, t):\n",
    "    \"\"\"Get mu_t-1 given x_t.\"\"\"\n",
    "    x = jnp.atleast_2d(x)\n",
    "    t = jnp.atleast_1d(t)\n",
    "\n",
    "    chex.assert_shape(x, (None, self._dim))\n",
    "    chex.assert_shape(t, (None,))\n",
    "    chex.assert_type(t, jnp.int64)\n",
    "\n",
    "    outs = self.net(x, t)\n",
    "    chex.assert_shape(outs, x.shape)\n",
    "    return outs\n",
    "\n",
    "  def stats(self):\n",
    "    \"\"\"Returns static variables for computing variances.\"\"\"\n",
    "    return {\n",
    "        'betas': self._betas,\n",
    "        'alphas': self._alphas,\n",
    "        'alphas_cumprod': self._alphas_cumprod,\n",
    "        'alphas_cumprod_prev': self._alphas_cumprod_prev,\n",
    "        'sqrt_alphas_cumprod': self._sqrt_alphas_cumprod,\n",
    "        'sqrt_one_minus_alphas_cumprod': self._sqrt_one_minus_alphas_cumprod,\n",
    "        'log_one_minus_alphas_cumprod': self._log_one_minus_alphas_cumprod,\n",
    "        'sqrt_recip_alphas_cumprod': self._sqrt_recip_alphas_cumprod,\n",
    "        'sqrt_recipm1_alphas_cumprod': self._sqrt_recipm1_alphas_cumprod,\n",
    "        'posterior_variance': self._posterior_variance,\n",
    "        'posterior_log_variace_clipped': self._posterior_log_variance_clipped\n",
    "    }\n",
    "\n",
    "  def q_mean_variance(self, x_0, t):\n",
    "    \"\"\"Returns parameters of q(x_t | x_0).\"\"\"\n",
    "    mean = extract(self._sqrt_alphas_cumprod, t, x_0.shape) * x_0\n",
    "    variance = extract(1. - self._alphas_cumprod, t, x_0.shape)\n",
    "    log_variance = extract(self._log_one_minus_alphas_cumprod, t, x_0.shape)\n",
    "    return mean, variance, log_variance\n",
    "\n",
    "  def q_sample(self, x_0, t, noise=None):\n",
    "    \"\"\"Sample from q(x_t | x_0).\"\"\"\n",
    "    chex.assert_shape(x_0, (None, self._dim))\n",
    "    chex.assert_shape(t, (None,))\n",
    "    chex.assert_type([x_0, t], [jnp.float32, jnp.int64])\n",
    "\n",
    "    if noise is None:\n",
    "      noise = jax.random.normal(hk.next_rng_key(), x_0.shape)\n",
    "\n",
    "    x_t = extract(self._sqrt_alphas_cumprod, t, x_0.shape) * x_0 + extract(\n",
    "        self._sqrt_one_minus_alphas_cumprod, t, x_0.shape) * noise\n",
    "    chex.assert_shape(x_t, x_0.shape)\n",
    "    return x_t\n",
    "\n",
    "  def p_loss_simple(self, x_0, t):\n",
    "    \"\"\"Training loss for given x_0 and t.\"\"\"\n",
    "    chex.assert_shape(x_0, (None, self._dim))\n",
    "    chex.assert_shape(t, (None,))\n",
    "    chex.assert_type([x_0, t], [jnp.float32, jnp.int64])\n",
    "\n",
    "    noise = jax.random.normal(hk.next_rng_key(), x_0.shape)\n",
    "    x_noise = self.q_sample(x_0, t, noise)\n",
    "    noise_recon = self.forward(x_noise, t)\n",
    "    chex.assert_shape(noise_recon, x_0.shape)\n",
    "    mse = jnp.square(noise_recon - noise)\n",
    "\n",
    "    chex.assert_shape(mse, (t.shape[0], self._dim))\n",
    "    mse = jnp.mean(mse, axis=1)  # avg over the output dimension\n",
    "\n",
    "    chex.assert_shape(mse, t.shape)\n",
    "    return mse\n",
    "\n",
    "  def p_loss_kl(self, x_0, t):\n",
    "    \"\"\"Training loss for given x_0 and t (KL-weighted).\"\"\"\n",
    "    chex.assert_shape(x_0, (None, self._dim))\n",
    "    chex.assert_shape(t, (None,))\n",
    "    chex.assert_type([x_0, t], [jnp.float32, jnp.int64])\n",
    "\n",
    "    x_t = self.q_sample(x_0, t)\n",
    "    q_mean, _, q_log_variance = self.q_posterior(x_0, x_t, t)\n",
    "    p_mean, _, p_log_variance = self.p_mean_variance(x_t, t)\n",
    "\n",
    "    dist_q = distrax.Normal(q_mean, jnp.exp(0.5 * q_log_variance))\n",
    "    def _loss(pmu, plogvar):\n",
    "      dist_p = distrax.Normal(pmu, jnp.exp(0.5 * plogvar))\n",
    "      kl = dist_q.kl_divergence(dist_p).mean(-1)\n",
    "      nll = -dist_p.log_prob(x_0).mean(-1)\n",
    "      return kl, nll, jnp.where(t == 0, nll, kl)\n",
    "\n",
    "    kl, nll, loss = _loss(p_mean, p_log_variance)\n",
    "\n",
    "    chex.assert_equal_shape([nll, kl])\n",
    "    chex.assert_shape(loss, (t.shape[0],))\n",
    "    return loss\n",
    "\n",
    "  def q_posterior(self, x_0, x_t, t):\n",
    "    \"\"\"Obtain parameters of q(x_{t-1} | x_0, x_t).\"\"\"\n",
    "    chex.assert_shape(x_0, (None, self._dim))\n",
    "    chex.assert_shape(t, (None,))\n",
    "    chex.assert_type([x_0, t], [jnp.float32, jnp.int64])\n",
    "\n",
    "    mean = (\n",
    "        extract(self._posterior_mean_coef1, t, x_t.shape) * x_0\n",
    "        + extract(self._posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "    )\n",
    "    var = extract(self._posterior_variance, t, x_t.shape)\n",
    "    log_var_clipped = extract(self._posterior_log_variance_clipped,\n",
    "                              t, x_t.shape)\n",
    "    chex.assert_equal_shape([var, log_var_clipped])\n",
    "    chex.assert_equal_shape([x_0, x_t, mean])\n",
    "    return mean, var, log_var_clipped\n",
    "\n",
    "  def predict_start_from_noise(self, x_t, t, noise):\n",
    "    \"\"\"Predict x_0 from x_t.\"\"\"\n",
    "    chex.assert_shape(x_t, (None, self._dim))\n",
    "    chex.assert_shape(t, (None,))\n",
    "    chex.assert_type([x_t, t], [jnp.float32, jnp.int64])\n",
    "\n",
    "    x_0 = (\n",
    "        extract(self._sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t\n",
    "        - extract(self._sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
    "    )\n",
    "    chex.assert_shape(x_0, x_t.shape)\n",
    "    return x_0\n",
    "\n",
    "  def p_mean_variance(self, x, t, clip=jnp.inf):\n",
    "    \"\"\"Parameters of p(x_{t-1} | x_t).\"\"\"\n",
    "    chex.assert_shape(x, (None, self._dim))\n",
    "    chex.assert_shape(t, (None,))\n",
    "    chex.assert_type([x, t], [jnp.float32, jnp.int64])\n",
    "\n",
    "    x_recon = jnp.clip(\n",
    "        self.predict_start_from_noise(x, t, noise=self.forward(x, t)), -clip,\n",
    "        clip)\n",
    "\n",
    "    mean, var, log_var = self.q_posterior(x_recon, x, t)\n",
    "\n",
    "    chex.assert_shape(var, (x.shape[0], 1))\n",
    "    chex.assert_equal_shape([var, log_var])\n",
    "    chex.assert_shape(mean, x.shape)\n",
    "\n",
    "    if self._var_type == 'beta_reverse':\n",
    "      pass\n",
    "    elif self._var_type == 'beta_forward':\n",
    "      var = extract(self._betas, t, x.shape)\n",
    "      log_var = jnp.log(var)\n",
    "    elif self._var_type == 'learned':\n",
    "      log_var = extract(self._out_logvar, t, x.shape)\n",
    "      var = jnp.exp(log_var)\n",
    "    else:\n",
    "      raise ValueError(f'{self._var_type} not recognised.')\n",
    "\n",
    "    chex.assert_shape(var, (x.shape[0], 1))\n",
    "    chex.assert_equal_shape([var, log_var])\n",
    "    chex.assert_shape(mean, (x.shape[0], x.shape[1]))\n",
    "    return mean, var, log_var\n",
    "\n",
    "  def p_sample(self, x, t, rng_key=None, clip=jnp.inf):\n",
    "    \"\"\"Sample from p(x_{t-1} | x_t).\"\"\"\n",
    "    chex.assert_shape(x, (None, self._dim))\n",
    "    chex.assert_shape(t, (None,))\n",
    "    chex.assert_type([x, t], [jnp.float32, jnp.int64])\n",
    "\n",
    "    mean, _, log_var = self.p_mean_variance(x, t, clip=clip)\n",
    "\n",
    "    if rng_key is None:\n",
    "      rng_key = hk.next_rng_key()\n",
    "    noise = jax.random.normal(rng_key, x.shape)\n",
    "\n",
    "    x_tm1 = mean + jnp.exp(0.5 * log_var) * noise\n",
    "    chex.assert_equal_shape([x, x_tm1])\n",
    "    return x_tm1\n",
    "\n",
    "  def _prior_kl(self, x_0):\n",
    "    \"\"\"KL(q_T(x) || p(x)).\"\"\"\n",
    "    t = jnp.ones((x_0.shape[0],), dtype=jnp.int64) * (self._n_steps - 1)\n",
    "    qt_mean, _, qt_log_variance = self.q_mean_variance(x_0, t)\n",
    "    qt_dist = distrax.Normal(qt_mean, jnp.exp(0.5 * qt_log_variance))\n",
    "    p_dist = distrax.Normal(jnp.zeros_like(qt_mean), jnp.ones_like(qt_mean))\n",
    "    kl = qt_dist.kl_divergence(p_dist).mean(-1)\n",
    "    chex.assert_shape(kl, (x_0.shape[0],))\n",
    "    return kl\n",
    "\n",
    "  def logpx(self, x_0):\n",
    "    \"\"\"Full elbo estimate of model.\"\"\"\n",
    "    e = self._prior_kl(x_0)\n",
    "    chex.assert_shape(e, (x_0.shape[0],))\n",
    "    n_repeats = self._n_steps * self._samples_per_step\n",
    "    e = e.repeat(n_repeats, axis=0) / n_repeats\n",
    "\n",
    "    kls = self.loss_all_t(x_0, loss_type='kl')\n",
    "    logpx = -(kls + e) * self._dim * self._n_steps\n",
    "    return {'logpx': logpx}\n",
    "\n",
    "  def sample(self, n, clip=jnp.inf):\n",
    "    \"\"\"Sample from p(x).\"\"\"\n",
    "    chex.assert_type(n, int)\n",
    "    rng_key = hk.next_rng_key()\n",
    "    rng_key, r = jax.random.split(rng_key)\n",
    "\n",
    "    x = jax.random.normal(r, (n, self._dim))\n",
    "    def body_fn(i, inputs):\n",
    "      rng_key, x = inputs\n",
    "      rng_key, r = jax.random.split(rng_key)\n",
    "      j = self._n_steps - 1 - i\n",
    "      t = jnp.ones((n,), dtype=jnp.int64) * j\n",
    "      x = self.p_sample(x, t, rng_key=r, clip=clip)\n",
    "      return rng_key, x\n",
    "\n",
    "    x = hk.fori_loop(0, self._n_steps, body_fn, (rng_key, x))[1]\n",
    "\n",
    "    chex.assert_shape(x, (n, self._dim))\n",
    "    return x\n",
    "\n",
    "  def loss(self, x):\n",
    "    if self._mc_loss:\n",
    "      return self.loss_mc(x, loss_type=self._loss_type)\n",
    "    else:\n",
    "      return self.loss_all_t(x, loss_type=self._loss_type)\n",
    "\n",
    "  def loss_mc(self, x, loss_type=None):\n",
    "    \"\"\"Compute training loss, uniformly sampling t's.\"\"\"\n",
    "    chex.assert_shape(x, (None, self._dim))\n",
    "\n",
    "    t = jax.random.randint(hk.next_rng_key(), (x.shape[0],), 0, self._n_steps)\n",
    "    if loss_type == 'simple':\n",
    "      loss = self.p_loss_simple(x, t)\n",
    "      #loss = self.p_loss_simple_cv(x, t)\n",
    "    elif loss_type == 'kl':\n",
    "      loss = self.p_loss_kl(x, t)\n",
    "    else:\n",
    "      raise ValueError(f'Unrecognized loss type: {loss_type}')\n",
    "\n",
    "    chex.assert_shape(loss, (x.shape[0],))\n",
    "    return loss\n",
    "\n",
    "  def loss_all_t(self, x, loss_type=None):\n",
    "    \"\"\"Compute training loss enumerated and averaged over all t's.\"\"\"\n",
    "    chex.assert_shape(x, (None, self._dim))\n",
    "    x = jnp.array(x)\n",
    "    t = jnp.concatenate([jnp.arange(0, self._n_steps)] * x.shape[0])\n",
    "    t = jnp.tile(t[None], (self._samples_per_step,)).reshape(-1)\n",
    "    x_r = jnp.tile(x[None], (self._n_steps * self._samples_per_step,)).reshape(-1, *x.shape[1:])\n",
    "    chex.assert_equal_shape_prefix((x_r, t), 1)\n",
    "\n",
    "    if loss_type == 'simple':\n",
    "      loss = self.p_loss_simple(x_r, t)\n",
    "    elif loss_type == 'kl':\n",
    "      loss = self.p_loss_kl(x_r, t)\n",
    "    else:\n",
    "      raise ValueError(f'Unrecognized loss type: {loss_type}')\n",
    "    return loss\n",
    "\n",
    "  def p_gradient(self, x, t, clip=jnp.inf):\n",
    "    \"\"\"Compute mean and variance of Gaussian reverse model p(x_{t-1} | x_t).\"\"\"\n",
    "    b = x.shape[0]\n",
    "    # chex.assert_axis_dimension(t, 0, b)\n",
    "    gradient = self.forward(x, t)\n",
    "    gradient = gradient * extract(self._sqrt_recipm1_alphas_cumprod_custom, t, gradient.shape)\n",
    "\n",
    "    return gradient\n",
    "\n",
    "  def p_energy(self, x, t, clip=jnp.inf):\n",
    "    \"\"\"Compute mean and variance of Gaussian reverse model p(x_{t-1} | x_t).\"\"\"\n",
    "    b = x.shape[0]\n",
    "    # chex.assert_axis_dimension(t, 0, b)\n",
    "\n",
    "    x = jnp.atleast_2d(x)\n",
    "    t = jnp.atleast_1d(t)\n",
    "\n",
    "    chex.assert_shape(x, (None, self._dim))\n",
    "    chex.assert_shape(t, (None,))\n",
    "    chex.assert_type(t, jnp.int64)\n",
    "\n",
    "    energy = self.net.neg_logp_unnorm(x, t)\n",
    "    energy = energy * extract(self._sqrt_recipm1_alphas_cumprod_custom, t, energy.shape)\n",
    "\n",
    "    return energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZX1onFvvrtr"
   },
   "source": [
    "Define different datasets to train diffusion models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "458357a6"
   },
   "outputs": [],
   "source": [
    "def toy_gmm(n_comp=8, std=0.075, radius=0.5):\n",
    "  \"\"\"Ring of 2D Gaussians. Returns energy and sample functions.\"\"\"\n",
    "\n",
    "  means_x = np.cos(2 * np.pi *\n",
    "                   np.linspace(0, (n_comp - 1) / n_comp, n_comp)).reshape(\n",
    "                       n_comp, 1, 1, 1)\n",
    "  means_y = np.sin(2 * np.pi *\n",
    "                   np.linspace(0, (n_comp - 1) / n_comp, n_comp)).reshape(\n",
    "                       n_comp, 1, 1, 1)\n",
    "  mean = radius * np.concatenate((means_x, means_y), axis=1)\n",
    "  weights = np.ones(n_comp) / n_comp\n",
    "\n",
    "  def nll(x):\n",
    "    means = jnp.array(mean.reshape((-1, 1, 2)))\n",
    "    c = np.log(n_comp * 2 * np.pi * std**2)\n",
    "    f = jax.nn.logsumexp(\n",
    "        jnp.sum(-0.5 * jnp.square((x - means) / std), axis=2), axis=0) + c\n",
    "    # f = f + np.log(2)\n",
    "\n",
    "    return f\n",
    "\n",
    "  def sample(n_samples):\n",
    "    toy_sample = np.zeros(0).reshape((0, 2, 1, 1))\n",
    "    sample_group_sz = np.random.multinomial(n_samples, weights)\n",
    "    for i in range(n_comp):\n",
    "      sample_group = mean[i] + std * np.random.randn(\n",
    "          2 * sample_group_sz[i]).reshape(-1, 2, 1, 1)\n",
    "      toy_sample = np.concatenate((toy_sample, sample_group), axis=0)\n",
    "      np.random.shuffle(toy_sample)\n",
    "    data = toy_sample[:, :, 0, 0]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "  return nll, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8dc4812d"
   },
   "outputs": [],
   "source": [
    "def toy_gauss(radius=0.5):\n",
    "  \"\"\"Ring of 2D Gaussians. Returns energy and sample functions.\"\"\"\n",
    "\n",
    "  std = radius\n",
    "\n",
    "  def nll(x):\n",
    "    c = np.log(2 * np.pi * std**2)\n",
    "    f = -0.5 * jnp.square((x) / std) + c\n",
    "\n",
    "    return f\n",
    "\n",
    "  def sample(n_samples):\n",
    "    data = np.random.randn(n_samples, 2) * radius\n",
    "    return data\n",
    "\n",
    "\n",
    "  return nll, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36afbd3f"
   },
   "outputs": [],
   "source": [
    "def toy_box(scale=1.0):\n",
    "  \"\"\"Ring of 2D Gaussians. Returns energy and sample functions.\"\"\"\n",
    "\n",
    "  def nll(x):\n",
    "    return 1\n",
    "\n",
    "  def sample(n_samples):\n",
    "    data = np.random.uniform(-scale, scale, (n_samples, 2))\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "  return nll, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0281a39a"
   },
   "outputs": [],
   "source": [
    "def bar(scale=0.2):\n",
    "  \"\"\"Ring of 2D Gaussians. Returns energy and sample functions.\"\"\"\n",
    "\n",
    "  def nll(x):\n",
    "    return 1\n",
    "\n",
    "  def sample(n_samples):\n",
    "    data = np.random.uniform(-1, 1, (n_samples, 2))\n",
    "    data[:, 0] = data[:, 0] * scale\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "  return nll, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "684570fc"
   },
   "outputs": [],
   "source": [
    "def bar_horizontal(scale=0.2):\n",
    "  \"\"\"Ring of 2D Gaussians. Returns energy and sample functions.\"\"\"\n",
    "\n",
    "  def nll(x):\n",
    "    return 1\n",
    "\n",
    "  def sample(n_samples):\n",
    "    data = np.random.uniform(-1, 1, (n_samples, 2))\n",
    "    data[:, 1] = data[:, 1] * scale\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "  return nll, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ee902f8"
   },
   "outputs": [],
   "source": [
    "def toy_gmm_left(n_comp=8, std=0.075, radius=0.5):\n",
    "  \"\"\"Ring of 2D Gaussians. Returns energy and sample functions.\"\"\"\n",
    "\n",
    "  means_x = np.cos(2 * np.pi *\n",
    "                   np.linspace(0, (n_comp - 1) / n_comp, n_comp)).reshape(\n",
    "                       n_comp, 1, 1, 1)\n",
    "  means_y = np.sin(2 * np.pi *\n",
    "                   np.linspace(0, (n_comp - 1) / n_comp, n_comp)).reshape(\n",
    "                       n_comp, 1, 1, 1)\n",
    "  mean = radius * np.concatenate((means_x, means_y), axis=1)\n",
    "  mean = mean[[0, 1, 2, 3]]\n",
    "\n",
    "  n_comp = mean.shape[0]\n",
    "\n",
    "  weights = np.ones(n_comp) / n_comp\n",
    "\n",
    "  def nll(x):\n",
    "    means = jnp.array(mean.reshape((-1, 1, 2)))\n",
    "    c = np.log(n_comp * 2 * np.pi * std**2)\n",
    "    f = jax.nn.logsumexp(\n",
    "        jnp.sum(-0.5 * jnp.square((x - means) / std), axis=2), axis=0) + c\n",
    "    # f = f + np.log(2)\n",
    "\n",
    "    return f\n",
    "\n",
    "  def sample(n_samples):\n",
    "    toy_sample = np.zeros(0).reshape((0, 2, 1, 1))\n",
    "    sample_group_sz = np.random.multinomial(n_samples, weights)\n",
    "    for i in range(n_comp):\n",
    "      sample_group = mean[i] + std * np.random.randn(\n",
    "          2 * sample_group_sz[i]).reshape(-1, 2, 1, 1)\n",
    "      toy_sample = np.concatenate((toy_sample, sample_group), axis=0)\n",
    "      np.random.shuffle(toy_sample)\n",
    "    data = toy_sample[:, :, 0, 0]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "  return nll, sample\n",
    "\n",
    "\n",
    "def toy_gmm_right(n_comp=8, std=0.075, radius=0.5):\n",
    "  \"\"\"Ring of 2D Gaussians. Returns energy and sample functions.\"\"\"\n",
    "\n",
    "  means_x = np.cos(2 * np.pi *\n",
    "                   np.linspace(0, (n_comp - 1) / n_comp, n_comp)).reshape(\n",
    "                       n_comp, 1, 1, 1)\n",
    "  means_y = np.sin(2 * np.pi *\n",
    "                   np.linspace(0, (n_comp - 1) / n_comp, n_comp)).reshape(\n",
    "                       n_comp, 1, 1, 1)\n",
    "  mean = radius * np.concatenate((means_x, means_y), axis=1)\n",
    "  mean = mean[[4, 5, 6, 7]]\n",
    "  n_comp = mean.shape[0]\n",
    "\n",
    "  weights = np.ones(n_comp) / n_comp\n",
    "\n",
    "  def nll(x):\n",
    "    means = jnp.array(mean.reshape((-1, 1, 2)))\n",
    "    c = np.log(n_comp * 2 * np.pi * std**2)\n",
    "    f = jax.nn.logsumexp(\n",
    "        jnp.sum(-0.5 * jnp.square((x - means) / std), axis=2), axis=0) + c\n",
    "    # f = f + np.log(2)\n",
    "\n",
    "    return f\n",
    "\n",
    "  def sample(n_samples):\n",
    "    toy_sample = np.zeros(0).reshape((0, 2, 1, 1))\n",
    "    sample_group_sz = np.random.multinomial(n_samples, weights)\n",
    "    for i in range(n_comp):\n",
    "      sample_group = mean[i] + std * np.random.randn(\n",
    "          2 * sample_group_sz[i]).reshape(-1, 2, 1, 1)\n",
    "      toy_sample = np.concatenate((toy_sample, sample_group), axis=0)\n",
    "      np.random.shuffle(toy_sample)\n",
    "    data = toy_sample[:, :, 0, 0]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "  return nll, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "443b1588"
   },
   "outputs": [],
   "source": [
    "def right_bar(scale=0.1):\n",
    "  \"\"\"Ring of 2D Gaussians. Returns energy and sample functions.\"\"\"\n",
    "\n",
    "  def nll(x):\n",
    "    return 1\n",
    "\n",
    "  def sample(n_samples):\n",
    "    data = np.random.uniform(-1, 1, (n_samples, 2))\n",
    "    data[:, 0] = data[:, 0] * scale + 0.2\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "  return nll, sample\n",
    "\n",
    "def left_bar(scale=0.1):\n",
    "  \"\"\"Ring of 2D Gaussians. Returns energy and sample functions.\"\"\"\n",
    "\n",
    "  def nll(x):\n",
    "    return 1\n",
    "\n",
    "  def sample(n_samples):\n",
    "    data = np.random.uniform(-1, 1, (n_samples, 2))\n",
    "    data[:, 0] = data[:, 0] * scale - 0.2\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "  return nll, sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyCPsJ5T0ZR-"
   },
   "source": [
    "Define Jax operations for different model compositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52a3f187"
   },
   "outputs": [],
   "source": [
    "rng_seq = hk.PRNGSequence(0)\n",
    "seed = next(rng_seq)\n",
    "\n",
    "def forward_fn_product():\n",
    "  net_one = ResnetDiffusionModel(n_steps=n_steps, n_layers=4, x_dim=data_dim, h_dim=128, emb_dim=32)\n",
    "\n",
    "  if ebm:\n",
    "    net_one = EBMDiffusionModel(net_one)\n",
    "\n",
    "  net_two = ResnetDiffusionModel(n_steps=n_steps, n_layers=4, x_dim=data_dim, h_dim=128, emb_dim=32)\n",
    "\n",
    "  if ebm:\n",
    "    net_two = EBMDiffusionModel(net_two)\n",
    "\n",
    "  dual_net = ProductEBMDiffusionModel(net_one, net_two)\n",
    "  ddpm = PortableDiffusionModel(data_dim, n_steps, dual_net, var_type=\"beta_forward\")\n",
    "\n",
    "  def logp_unnorm(x, t):\n",
    "    scale_e = ddpm.energy_scale(-2 - t)\n",
    "    t = jnp.ones((x.shape[0],), dtype=jnp.int32) * t\n",
    "    return -dual_net.neg_logp_unnorm(x, t) * scale_e\n",
    "\n",
    "  def _logpx(x):\n",
    "    return ddpm.logpx(x)[\"logpx\"]\n",
    "\n",
    "  if ebm:\n",
    "    return ddpm.loss, (ddpm.loss, ddpm.sample, _logpx, logp_unnorm, ddpm.p_gradient, ddpm.p_energy)\n",
    "  else:\n",
    "    return ddpm.loss, (ddpm.loss, ddpm.sample, _logpx, logp_unnorm, ddpm.p_gradient)\n",
    "\n",
    "forward_product = hk.multi_transform(forward_fn_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0104296"
   },
   "outputs": [],
   "source": [
    "def forward_fn_mixture():\n",
    "  net_one = ResnetDiffusionModel(n_steps=n_steps, n_layers=4, x_dim=data_dim, h_dim=128, emb_dim=32)\n",
    "\n",
    "  if ebm:\n",
    "    net_one = EBMDiffusionModel(net_one)\n",
    "\n",
    "  net_two = ResnetDiffusionModel(n_steps=n_steps, n_layers=4, x_dim=data_dim, h_dim=128, emb_dim=32)\n",
    "\n",
    "  if ebm:\n",
    "    net_two = EBMDiffusionModel(net_two)\n",
    "\n",
    "  dual_net = MixtureEBMDiffusionModel(net_one, net_two)\n",
    "  ddpm = PortableDiffusionModel(data_dim, n_steps, dual_net, var_type=\"beta_forward\")\n",
    "\n",
    "  def logp_unnorm(x, t):\n",
    "    scale_e = ddpm.energy_scale(-2 - t)\n",
    "    t = jnp.ones((x.shape[0],), dtype=jnp.int32) * t\n",
    "    return -dual_net.neg_logp_unnorm(x, t) * scale_e\n",
    "\n",
    "  def _logpx(x):\n",
    "    return ddpm.logpx(x)[\"logpx\"]\n",
    "\n",
    "  if ebm:\n",
    "    return ddpm.loss, (ddpm.loss, ddpm.sample, _logpx, logp_unnorm, ddpm.p_gradient, ddpm.p_energy)\n",
    "  else:\n",
    "    return ddpm.loss, (ddpm.loss, ddpm.sample, _logpx, logp_unnorm, ddpm.p_gradient)\n",
    "\n",
    "forward_mixture = hk.multi_transform(forward_fn_mixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0b7f40e2"
   },
   "outputs": [],
   "source": [
    "def forward_fn_negation():\n",
    "  net_one = ResnetDiffusionModel(n_steps=n_steps, n_layers=4, x_dim=data_dim, h_dim=128, emb_dim=32)\n",
    "\n",
    "  if ebm:\n",
    "    net_one = EBMDiffusionModel(net_one)\n",
    "\n",
    "  net_two = ResnetDiffusionModel(n_steps=n_steps, n_layers=4, x_dim=data_dim, h_dim=128, emb_dim=32)\n",
    "\n",
    "  if ebm:\n",
    "    net_two = EBMDiffusionModel(net_two)\n",
    "\n",
    "  dual_net = NegationEBMDiffusionModel(net_one, net_two)\n",
    "  ddpm = PortableDiffusionModel(data_dim, n_steps, dual_net, var_type=\"beta_forward\")\n",
    "\n",
    "  def logp_unnorm(x, t):\n",
    "    scale_e = ddpm.energy_scale(-2 - t)\n",
    "    t = jnp.ones((x.shape[0],), dtype=jnp.int32) * t\n",
    "    return -dual_net.neg_logp_unnorm(x, t) * scale_e\n",
    "\n",
    "  def _logpx(x):\n",
    "    return ddpm.logpx(x)[\"logpx\"]\n",
    "\n",
    "  if ebm:\n",
    "    return ddpm.loss, (ddpm.loss, ddpm.sample, _logpx, logp_unnorm, ddpm.p_gradient, ddpm.p_energy)\n",
    "  else:\n",
    "    return ddpm.loss, (ddpm.loss, ddpm.sample, _logpx, logp_unnorm, ddpm.p_gradient)\n",
    "\n",
    "forward_negation = hk.multi_transform(forward_fn_negation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30b67281"
   },
   "outputs": [],
   "source": [
    "if ebm:\n",
    "  _, dual_product_sample_fn, dual_product_nll, dual_product_logp_unorm_fn, dual_product_gradient_fn, dual_product_energy_fn = forward_product.apply\n",
    "else:\n",
    "  _, dual_product_sample_fn, dual_product_nll, dual_product_logp_unorm_fn, dual_product_gradient_fn = forward_product.apply\n",
    "\n",
    "dual_product_sample_fn = jax.jit(dual_product_sample_fn, static_argnums=2)\n",
    "dual_product_logp_unnorm_fn = jax.jit(dual_product_logp_unorm_fn)\n",
    "dual_product_gradient_fn = jax.jit(dual_product_gradient_fn)\n",
    "\n",
    "if ebm:\n",
    "  dual_product_energy_fn = jax.jit(dual_product_energy_fn)\n",
    "\n",
    "dual_product_nll = jax.jit(dual_product_nll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6f01bb33"
   },
   "outputs": [],
   "source": [
    "if ebm:\n",
    "  _, dual_mixture_sample_fn, dual_mixture_nll, dual_mixture_logp_unorm_fn, dual_mixture_gradient_fn, dual_mixture_energy_fn = forward_mixture.apply\n",
    "else:\n",
    "  _, dual_mixture_sample_fn, dual_mixture_nll, dual_mixture_logp_unorm_fn, dual_mixture_gradient_fn = forward_mixture.apply\n",
    "\n",
    "dual_mixture_sample_fn = jax.jit(dual_mixture_sample_fn, static_argnums=2)\n",
    "dual_mixture_logp_unnorm_fn = jax.jit(dual_mixture_logp_unorm_fn)\n",
    "dual_mixture_gradient_fn = jax.jit(dual_mixture_gradient_fn)\n",
    "\n",
    "if ebm:\n",
    "  dual_mixture_energy_fn = jax.jit(dual_mixture_energy_fn)\n",
    "\n",
    "dual_mixture_nll = jax.jit(dual_mixture_nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5f60b7cd"
   },
   "outputs": [],
   "source": [
    "if ebm:\n",
    "  _, dual_negation_sample_fn, dual_negation_nll, dual_negation_logp_unorm_fn, dual_negation_gradient_fn, dual_negation_energy_fn = forward_negation.apply\n",
    "else:\n",
    "  _, dual_negation_sample_fn, dual_negation_nll, dual_negation_logp_unorm_fn, dual_negation_gradient_fn = forward_negation.apply\n",
    "\n",
    "dual_negation_sample_fn = jax.jit(dual_negation_sample_fn, static_argnums=2)\n",
    "dual_negation_logp_unnorm_fn = jax.jit(dual_negation_logp_unorm_fn)\n",
    "dual_negation_gradient_fn = jax.jit(dual_negation_gradient_fn)\n",
    "\n",
    "if ebm:\n",
    "  dual_negation_energy_fn = jax.jit(dual_negation_energy_fn)\n",
    "\n",
    "dual_negation_nll = jax.jit(dual_negation_nll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVn3r_MyvcBD"
   },
   "source": [
    "Code for different MCMC Samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "614f67ff"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import distrax\n",
    "import haiku as hk\n",
    "# pylint: disable=g-bare-generic\n",
    "from typing import Callable, Optional, Tuple, Union, Dict\n",
    "Array = jnp.ndarray\n",
    "Scalar = Union[float, int]\n",
    "RandomKey = Array\n",
    "\n",
    "GradientTarget = Callable[[Array, Array], Array]\n",
    "InitialSampler = Callable[[Array], Array]\n",
    "\n",
    "class AnnealedULASampler:\n",
    "  \"\"\"Implements AIS with ULA\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               num_steps: int,\n",
    "               num_samples_per_step: int,\n",
    "               step_sizes: jnp.array,\n",
    "               initial_distribution: distrax.Distribution,\n",
    "               target_distribution,\n",
    "               gradient_function):\n",
    "    assert len(step_sizes) == num_steps, \"Must have as many stepsizes as intermediate distributions.\"\n",
    "    self._step_sizes = step_sizes\n",
    "    self._num_steps = num_steps\n",
    "    self._num_samples_per_step = num_samples_per_step\n",
    "    self._initial_distribution = initial_distribution\n",
    "    self._target_distribution = target_distribution\n",
    "    if target_distribution is None:\n",
    "      assert gradient_function is not None\n",
    "      self._gradient_function = gradient_function\n",
    "    else:\n",
    "      self._gradient_function = jax.grad(\n",
    "          lambda x, i: target_distribution(x, i).sum())\n",
    "\n",
    "    self._total_steps = self._num_samples_per_step * (self._num_steps - 1)\n",
    "\n",
    "  def transition_distribution(self, i, x):\n",
    "    ss = self._step_sizes[i]\n",
    "    std = (2 * ss) ** .5\n",
    "    grad = self._gradient_function(x, i)\n",
    "    mu = x + grad * ss\n",
    "    dist = distrax.MultivariateNormalDiag(mu, jnp.ones_like(mu) * std)\n",
    "    return dist\n",
    "\n",
    "  def sample(self, key: RandomKey, n_samples: int):\n",
    "    init_key, key = jax.random.split(key)\n",
    "    x = self._initial_distribution.sample(seed=init_key,\n",
    "                                          sample_shape=(n_samples,))\n",
    "    logw = -self._initial_distribution.log_prob(x)\n",
    "\n",
    "    inputs = (key, logw, x)\n",
    "    def body_fn(i, inputs):\n",
    "      key, logw, x = inputs\n",
    "      dist_ind = (i // self._num_samples_per_step)\n",
    "      dist_forward = self.transition_distribution(dist_ind, x)\n",
    "      sample_key, key = jax.random.split(key)\n",
    "      x_hat = dist_forward.sample(seed=sample_key)\n",
    "      dist_reverse = self.transition_distribution(dist_ind - 1, x_hat)\n",
    "      logw += dist_reverse.log_prob(x) - dist_forward.log_prob(x_hat)\n",
    "      x = x_hat\n",
    "      return key, logw, x\n",
    "\n",
    "    _, logw, x = jax.lax.fori_loop(0, self._total_steps, body_fn, inputs)\n",
    "\n",
    "    if self._target_distribution is not None:\n",
    "      logw += self._target_distribution(x, self._num_steps - 1)\n",
    "    else:\n",
    "      logw = None\n",
    "\n",
    "    return x, logw, None\n",
    "\n",
    "  def logp_raise(self, key: RandomKey, x: jnp.array):\n",
    "    logw = jnp.zeros((x.shape[0],))\n",
    "\n",
    "    inputs = (key, logw, x)\n",
    "    def body_fn(i, inputs):\n",
    "      key, logw, x = inputs\n",
    "      ind = i // self._num_samples_per_step\n",
    "      dist_ind = self._num_steps - 1 - ind\n",
    "      dist_reverse = self.transition_distribution(dist_ind - 1, x)\n",
    "      sample_key, key = jax.random.split(key)\n",
    "      x_hat = dist_reverse.sample(seed=sample_key)\n",
    "      dist_forward = self.transition_distribution(dist_ind, x_hat)\n",
    "      logw += dist_forward.log_prob(x) - dist_reverse.log_prob(x_hat)\n",
    "      x = x_hat\n",
    "      inputs = key, logw, x\n",
    "      return key, logw, x\n",
    "\n",
    "    _, logw, x = jax.lax.fori_loop(0, self._total_steps, body_fn, inputs)\n",
    "    logw += self._initial_distribution.log_prob(x)\n",
    "    return x, logw\n",
    "\n",
    "\n",
    "\n",
    "def leapfrog_step(x_0: Array,\n",
    "                  v_0: Array,\n",
    "                  gradient_target: GradientTarget,\n",
    "                  step_size: Array,\n",
    "                  mass_diag_sqrt: Array,\n",
    "                  num_steps: int):\n",
    "  \"\"\"Multiple leapfrog steps with no metropolis correction.\"\"\"\n",
    "  x_k = x_0\n",
    "  v_k = v_0\n",
    "  if mass_diag_sqrt is None:\n",
    "    mass_diag_sqrt = jnp.ones_like(x_k)\n",
    "\n",
    "  mass_diag = mass_diag_sqrt ** 2.\n",
    "\n",
    "  for _ in range(num_steps):  # Inefficient version - should combine half steps\n",
    "    v_k += 0.5 * step_size * gradient_target(x_k)  # half step in v\n",
    "    x_k += step_size * v_k / mass_diag  # Step in x\n",
    "    grad = gradient_target(x_k)\n",
    "    v_k += 0.5 * step_size * grad  # half step in v\n",
    "  return x_k, v_k\n",
    "\n",
    "\n",
    "\n",
    "class AnnealedUHASampler:\n",
    "  \"\"\"Implements AIS with ULA\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               num_steps: int,\n",
    "               num_samples_per_step: int,\n",
    "               step_sizes: jnp.array,\n",
    "               damping_coeff: int,\n",
    "               mass_diag_sqrt: int,\n",
    "               num_leapfrog_steps: int,\n",
    "               initial_distribution: distrax.Distribution,\n",
    "               target_distribution,\n",
    "               gradient_function):\n",
    "    assert len(step_sizes) == num_steps, \"Must have as many stepsizes as intermediate distributions.\"\n",
    "    self._damping_coeff = damping_coeff\n",
    "    self._mass_diag_sqrt = mass_diag_sqrt\n",
    "    self._step_sizes = step_sizes\n",
    "    self._num_steps = num_steps\n",
    "    self._num_leapfrog_steps = num_leapfrog_steps\n",
    "    self._num_samples_per_step = num_samples_per_step\n",
    "    self._initial_distribution = initial_distribution\n",
    "    self._target_distribution = target_distribution\n",
    "    if target_distribution is None:\n",
    "      assert gradient_function is not None\n",
    "      self._gradient_function = gradient_function\n",
    "    else:\n",
    "      self._gradient_function = jax.grad(\n",
    "          lambda x, i: target_distribution(x, i).sum())\n",
    "\n",
    "    self._total_steps = self._num_samples_per_step * (self._num_steps - 1)\n",
    "\n",
    "  def leapfrog_step(self, x, v, i):\n",
    "      step_size = self._step_sizes[i]\n",
    "      return leapfrog_step(x, v, lambda _x: self._gradient_function(_x, i), step_size, self._mass_diag_sqrt, self._num_leapfrog_steps)\n",
    "\n",
    "  def sample(self, key: RandomKey, n_samples: int):\n",
    "    key, x_key = jax.random.split(key)\n",
    "    x_k = self._initial_distribution.sample(seed=x_key, sample_shape=(n_samples,))\n",
    "\n",
    "    v_dist = distrax.MultivariateNormalDiag(\n",
    "        loc=jnp.zeros_like(x_k),\n",
    "        scale_diag=jnp.ones_like(x_k) * self._mass_diag_sqrt)\n",
    "\n",
    "    key, v_key = jax.random.split(key)\n",
    "    v_k = v_dist.sample(seed=v_key)\n",
    "\n",
    "    logw = -self._initial_distribution.log_prob(x_k)\n",
    "\n",
    "    print(x_k.shape, v_k.shape, logw.shape)\n",
    "\n",
    "    inputs = (key, logw, x_k, v_k)\n",
    "    def body_fn(i, inputs):\n",
    "      # unpack inputs\n",
    "      key, logw, x_k, v_k = inputs\n",
    "      dist_ind = (i // self._num_samples_per_step)\n",
    "\n",
    "      eps_key, key = jax.random.split(key)\n",
    "      eps = jax.random.normal(eps_key, x_k.shape)\n",
    "      # resample momentum\n",
    "      v_k_prime = v_k * self._damping_coeff + jnp.sqrt(1. - self._damping_coeff**2) * eps * self._mass_diag_sqrt\n",
    "      # advance samples\n",
    "      x_k, v_k = self.leapfrog_step(x_k, v_k_prime, dist_ind)\n",
    "      # compute change in density\n",
    "      logp_v_p = v_dist.log_prob(v_k_prime)\n",
    "      logp_v = v_dist.log_prob(v_k)\n",
    "      # update importance weights\n",
    "      logw += logp_v - logp_v_p\n",
    "      return key, logw, x_k, v_k\n",
    "    _, logw, x_k, v_k = jax.lax.fori_loop(0, self._total_steps, body_fn, inputs)\n",
    "\n",
    "    if self._target_distribution is not None:\n",
    "      logw += self._target_distribution(x_k, self._num_steps - 1)\n",
    "    else:\n",
    "      logw = None\n",
    "\n",
    "    return x_k, logw, None\n",
    "\n",
    "  def logp_raise(self, key: RandomKey, x: jnp.array):\n",
    "    logw = jnp.zeros((x.shape[0],))\n",
    "    x_k = x\n",
    "    v_dist = distrax.MultivariateNormalDiag(\n",
    "        loc=jnp.zeros_like(x_k),\n",
    "        scale_diag=jnp.ones_like(x_k) * self._mass_diag_sqrt)\n",
    "\n",
    "    key, v_key = jax.random.split(key)\n",
    "    v_k = v_dist.sample(seed=v_key)\n",
    "\n",
    "    inputs = (key, logw, x_k, v_k)\n",
    "    def body_fn(i, inputs):\n",
    "      key, logw, x_k, v_k = inputs\n",
    "      ind = i // self._num_samples_per_step\n",
    "      dist_ind = self._num_steps - 1 - ind - 1\n",
    "\n",
    "      eps_key, key = jax.random.split(key)\n",
    "      eps = jax.random.normal(eps_key, x_k.shape)\n",
    "      # resample momentum\n",
    "      v_k_prime = v_k * self._damping_coeff + jnp.sqrt(1. - self._damping_coeff**2) * eps * self._mass_diag_sqrt\n",
    "      # advance samples\n",
    "      x_k, v_k = self.leapfrog_step(x_k, v_k_prime, dist_ind)\n",
    "\n",
    "      logp_v_p = v_dist.log_prob(v_k_prime)\n",
    "      logp_v = v_dist.log_prob(v_k)\n",
    "      # update importance weights\n",
    "      logw += logp_v - logp_v_p\n",
    "      return key, logw, x_k, v_k\n",
    "    _, logw, x_k, v_k = jax.lax.fori_loop(0, self._total_steps, body_fn, inputs)\n",
    "\n",
    "    logw += self._initial_distribution.log_prob(x_k)\n",
    "    return x_k, logw\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ede8d5e1"
   },
   "outputs": [],
   "source": [
    "class AnnealedMALASampler:\n",
    "  \"\"\"Implements AIS with MALA\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               num_steps: int,\n",
    "               num_samples_per_step: int,\n",
    "               step_sizes: jnp.array,\n",
    "               initial_distribution: distrax.Distribution,\n",
    "               target_distribution,\n",
    "               gradient_function,\n",
    "               energy_function):\n",
    "    assert len(step_sizes) == num_steps, \"Must have as many stepsizes as intermediate distributions.\"\n",
    "    self._step_sizes = step_sizes\n",
    "    self._num_steps = num_steps\n",
    "    self._num_samples_per_step = num_samples_per_step\n",
    "    self._initial_distribution = initial_distribution\n",
    "    self._target_distribution = target_distribution\n",
    "\n",
    "    self._gradient_function = gradient_function\n",
    "    self._energy_function = energy_function\n",
    "\n",
    "    self._total_steps = self._num_samples_per_step * (self._num_steps)\n",
    "    self._total_steps_reverse = self._num_samples_per_step * self._num_steps\n",
    "\n",
    "  def transition_distribution(self, i, x):\n",
    "    ss = self._step_sizes[i]\n",
    "    std = (2 * ss) ** .5\n",
    "    grad = self._gradient_function(x, i)\n",
    "    mu = x + grad * ss\n",
    "    dist = distrax.MultivariateNormalDiag(mu, jnp.ones_like(mu) * std)\n",
    "    return dist\n",
    "\n",
    "  def sample(self, key: RandomKey, n_samples: int):\n",
    "    init_key, key = jax.random.split(key)\n",
    "    x = self._initial_distribution.sample(seed=init_key,\n",
    "                                          sample_shape=(n_samples,))\n",
    "    logw = -self._initial_distribution.log_prob(x)\n",
    "\n",
    "    accept_rate = jnp.zeros((self._num_steps,))\n",
    "    inputs = (key, logw, x, accept_rate)\n",
    "    def body_fn(i, inputs):\n",
    "      # setup\n",
    "      key, logw, x, accept_rate = inputs\n",
    "      dist_ind = (i // self._num_samples_per_step)\n",
    "      sample_key, accept_key, key = jax.random.split(key, 3)\n",
    "      # compute forward distribution and sample\n",
    "      dist_forward = self.transition_distribution(dist_ind, x)\n",
    "      x_hat = dist_forward.sample(seed=sample_key)\n",
    "      # compute reverse distribution\n",
    "      dist_reverse = self.transition_distribution(dist_ind, x_hat)\n",
    "      # compute previous and current logp(x)\n",
    "      logp_x = self._energy_function(x, dist_ind)\n",
    "      logp_x_hat = self._energy_function(x_hat, dist_ind)\n",
    "      # compute proposal and reversal probs\n",
    "      logp_reverse = dist_reverse.log_prob(x)\n",
    "      logp_forward = dist_forward.log_prob(x_hat)\n",
    "      # accept prob\n",
    "      logp_accept = logp_x_hat - logp_x + logp_reverse - logp_forward\n",
    "      u = jax.random.uniform(accept_key, (x.shape[0],))\n",
    "      accept = (u < jnp.exp(logp_accept)).astype(jnp.float32)\n",
    "      # update samples and importance weights\n",
    "      x = accept[:, None] * x_hat + (1 - accept[:, None]) * x\n",
    "      logw += (logp_x - logp_x_hat) * accept\n",
    "      # update accept rate\n",
    "      accept_rate = accept_rate.at[dist_ind].set(accept_rate[dist_ind] + accept.mean())\n",
    "      return key, logw, x, accept_rate\n",
    "\n",
    "    _, logw, x, accept_rate = jax.lax.fori_loop(0, self._total_steps, body_fn, inputs)\n",
    "    accept_rate /= self._num_samples_per_step\n",
    "\n",
    "    # logw += self._target_distribution(x, self._num_steps - 1)\n",
    "    return x, logw, accept_rate\n",
    "\n",
    "  def logp_raise(self, key: RandomKey, x: jnp.array):\n",
    "    logw = jnp.zeros((x.shape[0],))\n",
    "    accept_rate = jnp.zeros((self._num_steps,))\n",
    "\n",
    "    inputs = (key, logw, x, accept_rate)\n",
    "    def body_fn(i, inputs):\n",
    "      # setup\n",
    "      key, logw, x, accept_rate = inputs\n",
    "      ind = i // self._num_samples_per_step\n",
    "      dist_ind = self._num_steps - 1 - ind\n",
    "      sample_key, accept_key, key = jax.random.split(key, 3)\n",
    "      # compute reverse distribution and sample\n",
    "      dist_reverse = self.transition_distribution(dist_ind, x)\n",
    "      x_hat = dist_reverse.sample(seed=sample_key)\n",
    "      # compute the forward distribution\n",
    "      dist_forward = self.transition_distribution(dist_ind, x_hat)\n",
    "      # compute previous and current logp(x)\n",
    "      logp_x = self._target_distribution(x, dist_ind)\n",
    "      logp_x_hat = self._target_distribution(x_hat, dist_ind)\n",
    "      # compute proposal and reversal probs\n",
    "      logp_reverse = dist_reverse.log_prob(x_hat)\n",
    "      logp_forward = dist_forward.log_prob(x)\n",
    "      # accept prob\n",
    "      logp_accept = logp_x_hat - logp_x + logp_forward - logp_reverse\n",
    "      u = jax.random.uniform(accept_key, (x.shape[0],))\n",
    "      accept = (u < jnp.exp(logp_accept)).astype(jnp.float32)\n",
    "      # update samples and importance weights\n",
    "      x = accept[:, None] * x_hat + (1 - accept[:, None]) * x\n",
    "      logw += (logp_x - logp_x_hat) * accept\n",
    "      # update accept rate\n",
    "      accept_rate = accept_rate.at[dist_ind].set(accept_rate[dist_ind] + accept.mean())\n",
    "      return key, logw, x, accept_rate\n",
    "\n",
    "    _, logw, x, accept_rate = jax.lax.fori_loop(0, self._total_steps_reverse, body_fn, inputs)\n",
    "    accept_rate /= self._num_samples_per_step\n",
    "    logw += self._initial_distribution.log_prob(x)\n",
    "    return x, logw, accept_rate\n",
    "\n",
    "def update_step_sizes(step_sizes, accept_rate, optimal_acc_rate=.57, lr_step_size=0.02):\n",
    "  return step_sizes*(1.0 + lr_step_size*(accept_rate - optimal_acc_rate))\n",
    "\n",
    "\n",
    "class AnnealedMUHASampler:\n",
    "  \"\"\"Implements AIS with ULA\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               num_steps: int,\n",
    "               num_samples_per_step: int,\n",
    "               step_sizes: jnp.array,\n",
    "               damping_coeff: int,\n",
    "               mass_diag_sqrt: float,\n",
    "               num_leapfrog_steps: int,\n",
    "               initial_distribution: distrax.Distribution,\n",
    "               target_distribution,\n",
    "               gradient_function,\n",
    "               energy_function):\n",
    "    assert len(step_sizes) == num_steps, \"Must have as many stepsizes as intermediate distributions.\"\n",
    "    self._damping_coeff = damping_coeff\n",
    "    self._mass_diag_sqrt = mass_diag_sqrt\n",
    "    self._step_sizes = step_sizes\n",
    "    self._num_steps = num_steps\n",
    "    self._num_leapfrog_steps = num_leapfrog_steps\n",
    "    self._num_samples_per_step = num_samples_per_step\n",
    "    self._initial_distribution = initial_distribution\n",
    "    self._gradient_function = gradient_function\n",
    "    self._energy_function = energy_function\n",
    "\n",
    "    self._total_steps = self._num_samples_per_step * (self._num_steps - 1)\n",
    "    self._total_steps_reverse = self._num_samples_per_step * self._num_steps\n",
    "\n",
    "  def leapfrog_step(self, x, v, i):\n",
    "      step_size = self._step_sizes[i]\n",
    "      return leapfrog_step(x, v, lambda _x: self._gradient_function(_x, i), step_size, self._mass_diag_sqrt, self._num_leapfrog_steps)\n",
    "\n",
    "  def sample(self, key: RandomKey, n_samples: int):\n",
    "    key, x_key = jax.random.split(key)\n",
    "    x_k = self._initial_distribution.sample(seed=x_key, sample_shape=(n_samples,))\n",
    "\n",
    "    v_dist = distrax.MultivariateNormalDiag(\n",
    "        loc=jnp.zeros_like(x_k),\n",
    "        scale_diag=jnp.ones_like(x_k) * self._mass_diag_sqrt)\n",
    "\n",
    "    key, v_key = jax.random.split(key)\n",
    "    v_k = v_dist.sample(seed=v_key)\n",
    "\n",
    "    logw = -self._initial_distribution.log_prob(x_k)\n",
    "\n",
    "    accept_rate = jnp.zeros((self._num_steps,))\n",
    "    inputs = (key, logw, x_k, v_k, accept_rate)\n",
    "    def body_fn(i, inputs):\n",
    "      # unpack inputs\n",
    "      key, logw, x_k, v_k, accept_rate = inputs\n",
    "      dist_ind = (i // self._num_samples_per_step) + 1\n",
    "      eps_key, accept_key, key = jax.random.split(key, 3)\n",
    "      eps = jax.random.normal(eps_key, x_k.shape)\n",
    "      # resample momentum\n",
    "      v_k_prime = v_k * self._damping_coeff + jnp.sqrt(1. - self._damping_coeff**2) * eps * self._mass_diag_sqrt\n",
    "      # advance samples\n",
    "      x_k_next, v_k_next = self.leapfrog_step(x_k, v_k_prime, dist_ind)\n",
    "      # compute change in density\n",
    "      logp_v_p = v_dist.log_prob(v_k_prime)\n",
    "      logp_v = v_dist.log_prob(v_k_next)\n",
    "      # compute target log-probs\n",
    "      logp_x = self._energy_function(x_k, dist_ind)\n",
    "      logp_x_hat = self._energy_function(x_k_next, dist_ind)\n",
    "      # compute joint log-probs\n",
    "      log_joint_prev = logp_x + logp_v_p\n",
    "      log_joint_next = logp_x_hat + logp_v\n",
    "      # acceptance prob\n",
    "      logp_accept = log_joint_next - log_joint_prev\n",
    "      u = jax.random.uniform(accept_key, (x_k_next.shape[0],))\n",
    "      accept = (u < jnp.exp(logp_accept)).astype(jnp.float32)\n",
    "      # update importance weights\n",
    "      logw += (logp_x - logp_x_hat) * accept\n",
    "      # update samples\n",
    "      x_k = accept[:, None] * x_k_next + (1 - accept[:, None]) * x_k\n",
    "      v_k = accept[:, None] * v_k_next + (1 - accept[:, None]) * v_k_prime\n",
    "      accept_rate = accept_rate.at[dist_ind].set(accept_rate[dist_ind] + accept.mean())\n",
    "      return key, logw, x_k, v_k, accept_rate\n",
    "    _, logw, x_k, v_k, accept_rate = jax.lax.fori_loop(0, self._total_steps, body_fn, inputs)\n",
    "\n",
    "    # logw += self._target_distribution(x_k, self._num_steps - 1)\n",
    "    accept_rate /= self._num_samples_per_step\n",
    "    return x_k, logw, accept_rate\n",
    "\n",
    "  def logp_raise(self, key: RandomKey, x: jnp.array):\n",
    "    logw = jnp.zeros((x.shape[0],))\n",
    "    x_k = x\n",
    "    v_dist = distrax.MultivariateNormalDiag(\n",
    "        loc=jnp.zeros_like(x_k),\n",
    "        scale_diag=jnp.ones_like(x_k) * self._mass_diag_sqrt)\n",
    "\n",
    "    key, v_key = jax.random.split(key)\n",
    "    v_k = v_dist.sample(seed=v_key)\n",
    "\n",
    "    accept_rate = jnp.zeros((self._num_steps,))\n",
    "\n",
    "    inputs = (key, logw, x_k, v_k, accept_rate)\n",
    "    def body_fn(i, inputs):\n",
    "      key, logw, x_k, v_k, accept_rate = inputs\n",
    "      ind = i // self._num_samples_per_step\n",
    "      dist_ind = self._num_steps - 1 - ind\n",
    "\n",
    "      eps_key, accept_key, key = jax.random.split(key, 3)\n",
    "      eps = jax.random.normal(eps_key, x_k.shape)\n",
    "      # resample momentum\n",
    "      v_k_prime = v_k * self._damping_coeff + jnp.sqrt(1. - self._damping_coeff**2) * eps * self._mass_diag_sqrt\n",
    "      # advance samples\n",
    "      x_k_next, v_k_next = self.leapfrog_step(x_k, v_k_prime, dist_ind)\n",
    "      # compute change in density\n",
    "      logp_v_p = v_dist.log_prob(v_k_prime)\n",
    "      logp_v = v_dist.log_prob(v_k_next)\n",
    "      # compute target log-probs\n",
    "      logp_x = self._target_distribution(x_k, dist_ind)\n",
    "      logp_x_hat = self._target_distribution(x_k_next, dist_ind)\n",
    "      # compute joint log-probs\n",
    "      log_joint_prev = logp_x + logp_v_p\n",
    "      log_joint_next = logp_x_hat + logp_v\n",
    "      # acceptance prob\n",
    "      logp_accept = log_joint_next - log_joint_prev\n",
    "      u = jax.random.uniform(accept_key, (x.shape[0],))\n",
    "      accept = (u < jnp.exp(logp_accept)).astype(jnp.float32)\n",
    "      logw += (logp_x - logp_x_hat) * accept\n",
    "      # update samples\n",
    "      x_k = accept[:, None] * x_k_next + (1 - accept[:, None]) * x_k\n",
    "      v_k = accept[:, None] * v_k_next + (1 - accept[:, None]) * v_k_prime\n",
    "      accept_rate = accept_rate.at[dist_ind].set(accept_rate[dist_ind] + accept.mean())\n",
    "      return key, logw, x_k, v_k, accept_rate\n",
    "    _, logw, x_k, v_k, accept_rate = jax.lax.fori_loop(0, self._total_steps_reverse, body_fn, inputs)\n",
    "\n",
    "    logw += self._initial_distribution.log_prob(x_k)\n",
    "    accept_rate /= self._num_samples_per_step\n",
    "    return x_k, logw, accept_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RM3_NNjEzjSN"
   },
   "source": [
    "Execute conjunction composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e43533bd"
   },
   "outputs": [],
   "source": [
    "# Train Spiral EBM Model\n",
    "batch_size = 1000\n",
    "data_dim = 2\n",
    "num_steps = 15001\n",
    "\n",
    "EMA = .999\n",
    "\n",
    "n_steps = 100\n",
    "net_params = {\"n_layers\": 4,\n",
    "              \"h_dim\": 128,\n",
    "              \"emb_dim\": 32}\n",
    "\n",
    "def forward_fn():\n",
    "  net = ResnetDiffusionModel(n_steps=n_steps, n_layers=4, x_dim=data_dim, h_dim=128, emb_dim=32)\n",
    "\n",
    "  if ebm:\n",
    "    net = EBMDiffusionModel(net)\n",
    "\n",
    "  ddpm = PortableDiffusionModel(data_dim, n_steps, net, var_type=\"beta_forward\")\n",
    "\n",
    "  def logp_unnorm(x, t):\n",
    "    scale_e = ddpm.energy_scale(-2 - t)\n",
    "    t = jnp.ones((x.shape[0],), dtype=jnp.int32) * t\n",
    "    return -net.neg_logp_unnorm(x, t) * scale_e\n",
    "\n",
    "  def _logpx(x):\n",
    "    return ddpm.logpx(x)[\"logpx\"]\n",
    "  return ddpm.loss, (ddpm.loss, ddpm.sample, _logpx, logp_unnorm)\n",
    "\n",
    "forward = hk.multi_transform(forward_fn)\n",
    "rng_seq = hk.PRNGSequence(0)\n",
    "\n",
    "xr = [-.75, .75]\n",
    "yr = [-.75, .75]\n",
    "\n",
    "\n",
    "def plot_samples(x):\n",
    "  plt.scatter(x[:, 0], x[:, 1])\n",
    "  plt.xlim(-2, 2)\n",
    "  plt.ylim(-2, 2)\n",
    "\n",
    "def dist_show_2d(fn, xr, yr):\n",
    "    nticks = 100\n",
    "    x, y = np.meshgrid(np.linspace(xr[0], xr[1], nticks), np.linspace(yr[0], yr[1], nticks))\n",
    "    coord = np.stack([x, y], axis=-1).reshape((-1, 2))\n",
    "    heatmap = fn(coord).reshape((nticks, nticks))\n",
    "    plt.imshow(heatmap)\n",
    "\n",
    "# load data\n",
    "dataset_energy, dataset_sample = toy_gmm(std=.03)\n",
    "x = dataset_sample(batch_size)\n",
    "\n",
    "plot_samples(x)\n",
    "plt.show()\n",
    "x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "\n",
    "params = forward.init(next(rng_seq), x)\n",
    "loss_fn, sample_fn, logpx_fn, logp_unnorm_fn = forward.apply\n",
    "param_count = sum(x.size for x in jax.tree_leaves(params))\n",
    "for k, v in jax.tree_map(lambda x: x.shape, params).items():\n",
    "  print(k, v)\n",
    "print(\"Model has {} params\".format(param_count))\n",
    "\n",
    "\n",
    "opt = optax.adam(1e-3)\n",
    "opt_state = opt.init(params)\n",
    "\n",
    "sample_fn = jax.jit(sample_fn, static_argnums=2)\n",
    "logpx_fn = jax.jit(logpx_fn)\n",
    "\n",
    "logp_unnorm_fn = jax.jit(logp_unnorm_fn)\n",
    "\n",
    "@jax.jit\n",
    "def mean_loss_fn(params, rng, x):\n",
    "  loss = loss_fn(params, rng, x)\n",
    "  return loss.mean()\n",
    "\n",
    "@jax.jit\n",
    "def update(params, opt_state, rng, x):\n",
    "  loss, grad = jax.value_and_grad(mean_loss_fn)(params, rng, x)\n",
    "\n",
    "  updates, opt_state = opt.update(grad, opt_state)\n",
    "  new_params = optax.apply_updates(params, updates)\n",
    "  return loss, new_params, opt_state\n",
    "\n",
    "ema_params = params\n",
    "losses = []\n",
    "test_logpx = []\n",
    "itr = 0\n",
    "\n",
    "for itr in range(num_steps):\n",
    "  x = dataset_sample(batch_size)\n",
    "\n",
    "  x = x.reshape(x.shape[0], -1)\n",
    "  start_time = time.time()\n",
    "  loss, params, opt_state = update(params, opt_state, next(rng_seq), x)\n",
    "  duration_update = time.time() - start_time\n",
    "  ema_params = jax.tree_map(lambda e, p: e * EMA + p * (1 - EMA), ema_params, params)\n",
    "\n",
    "  if itr % 100 == 0:\n",
    "    print(itr, loss, \"time:\", duration_update)\n",
    "    losses.append(loss)\n",
    "  if itr % 1000 == 0:\n",
    "    x_samp = sample_fn(ema_params, next(rng_seq), batch_size)\n",
    "    plot_samples(x_samp)\n",
    "    plt.show()\n",
    "    logpx = logpx_fn(ema_params, next(rng_seq), x).mean()\n",
    "    print(\"TEST\", itr, 'logpx', logpx)\n",
    "    test_logpx.append(logpx)\n",
    "\n",
    "    if ebm:\n",
    "      for t in range(10):\n",
    "        dist_show_2d(lambda x: logp_unnorm_fn(ema_params, next(rng_seq), x, 10*t), xr=xr, yr=yr)\n",
    "        plt.show()\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "plt.plot(test_logpx)\n",
    "\n",
    "flow_params = params\n",
    "flow_rng = next(rng_seq)\n",
    "\n",
    "spiral_params = ema_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2ab0131"
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "num_steps = 15001\n",
    "\n",
    "EMA = .999\n",
    "\n",
    "\n",
    "net_params = {\"n_layers\": 4,\n",
    "              \"h_dim\": 128,\n",
    "              \"emb_dim\": 32}\n",
    "\n",
    "def forward_fn():\n",
    "  net = ResnetDiffusionModel(n_steps=n_steps, n_layers=4, x_dim=data_dim, h_dim=128, emb_dim=32)\n",
    "\n",
    "  if ebm:\n",
    "    net = EBMDiffusionModel(net)\n",
    "\n",
    "  ddpm = PortableDiffusionModel(data_dim, n_steps, net, var_type=\"beta_forward\")\n",
    "\n",
    "  def logp_unnorm(x, t):\n",
    "    scale_e = ddpm.energy_scale(-2 - t)\n",
    "    t = jnp.ones((x.shape[0],), dtype=jnp.int32) * t\n",
    "    return -net.neg_logp_unnorm(x, t) * scale_e\n",
    "\n",
    "  def _logpx(x):\n",
    "    return ddpm.logpx(x)[\"logpx\"]\n",
    "  return ddpm.loss, (ddpm.loss, ddpm.sample, _logpx, logp_unnorm)\n",
    "\n",
    "forward = hk.multi_transform(forward_fn)\n",
    "rng_seq = hk.PRNGSequence(0)\n",
    "\n",
    "xr = [-.75, .75]\n",
    "yr = [-.75, .75]\n",
    "\n",
    "\n",
    "def plot_samples(x):\n",
    "  plt.scatter(x[:, 0], x[:, 1])\n",
    "  plt.xlim(-2, 2)\n",
    "  plt.ylim(-2, 2)\n",
    "\n",
    "def dist_show_2d(fn, xr, yr):\n",
    "    nticks = 100\n",
    "    x, y = np.meshgrid(np.linspace(xr[0], xr[1], nticks), np.linspace(yr[0], yr[1], nticks))\n",
    "    coord = np.stack([x, y], axis=-1).reshape((-1, 2))\n",
    "    heatmap = fn(coord).reshape((nticks, nticks))\n",
    "    plt.imshow(heatmap)\n",
    "\n",
    "# load data\n",
    "dataset_energy, dataset_sample = bar(scale=0.2)\n",
    "x = dataset_sample(batch_size)\n",
    "\n",
    "plot_samples(x)\n",
    "plt.show()\n",
    "x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "\n",
    "params = forward.init(next(rng_seq), x)\n",
    "loss_fn, sample_fn, logpx_fn, logp_unnorm_fn = forward.apply\n",
    "param_count = sum(x.size for x in jax.tree_leaves(params))\n",
    "for k, v in jax.tree_map(lambda x: x.shape, params).items():\n",
    "  print(k, v)\n",
    "print(\"Model has {} params\".format(param_count))\n",
    "\n",
    "\n",
    "opt = optax.adam(1e-3)\n",
    "opt_state = opt.init(params)\n",
    "\n",
    "sample_fn = jax.jit(sample_fn, static_argnums=2)\n",
    "logpx_fn = jax.jit(logpx_fn)\n",
    "\n",
    "logp_unnorm_fn = jax.jit(logp_unnorm_fn)\n",
    "\n",
    "@jax.jit\n",
    "def mean_loss_fn(params, rng, x):\n",
    "  loss = loss_fn(params, rng, x)\n",
    "  return loss.mean()\n",
    "\n",
    "@jax.jit\n",
    "def update(params, opt_state, rng, x):\n",
    "  loss, grad = jax.value_and_grad(mean_loss_fn)(params, rng, x)\n",
    "\n",
    "  updates, opt_state = opt.update(grad, opt_state)\n",
    "  new_params = optax.apply_updates(params, updates)\n",
    "  return loss, new_params, opt_state\n",
    "\n",
    "ema_params = params\n",
    "losses = []\n",
    "test_logpx = []\n",
    "itr = 0\n",
    "\n",
    "for itr in range(num_steps):\n",
    "  x = dataset_sample(batch_size)\n",
    "\n",
    "  x = x.reshape(x.shape[0], -1)\n",
    "  start_time = time.time()\n",
    "  loss, params, opt_state = update(params, opt_state, next(rng_seq), x)\n",
    "  duration_update = time.time() - start_time\n",
    "  ema_params = jax.tree_map(lambda e, p: e * EMA + p * (1 - EMA), ema_params, params)\n",
    "\n",
    "  if itr % 100 == 0:\n",
    "    print(itr, loss, \"time:\", duration_update)\n",
    "    losses.append(loss)\n",
    "  if itr % 1000 == 0:\n",
    "    x_samp = sample_fn(ema_params, next(rng_seq), batch_size)\n",
    "    plot_samples(x_samp)\n",
    "    plt.show()\n",
    "    logpx = logpx_fn(ema_params, next(rng_seq), x).mean()\n",
    "    print(\"TEST\", itr, 'logpx', logpx)\n",
    "    test_logpx.append(logpx)\n",
    "\n",
    "    if ebm:\n",
    "      for t in range(10):\n",
    "        dist_show_2d(lambda x: logp_unnorm_fn(ema_params, next(rng_seq), x, 10*t), xr=xr, yr=yr)\n",
    "        plt.show()\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "plt.plot(test_logpx)\n",
    "\n",
    "bar_params = ema_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ac57425"
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "\n",
    "for k, v in spiral_params.items():\n",
    "  params[k] = v\n",
    "\n",
    "for k, v in bar_params.items():\n",
    "  k = k.replace('resnet_diffusion_model/', 'resnet_diffusion_model_1/')\n",
    "  params[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "60f00d2f",
    "outputId": "fde62a6b-ba31-47c7-c1eb-95e61637e9e3"
   },
   "outputs": [],
   "source": [
    "# Sampling from product of distributions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dim = 2\n",
    "n_mode = 4\n",
    "std = .05\n",
    "init_std = 1.\n",
    "init_mu = 0.\n",
    "n_steps = 10\n",
    "damping = .5\n",
    "mass_diag_sqrt = 1.\n",
    "num_leapfrog = 3\n",
    "samples_per_step = 10\n",
    "uha_step_size = .03\n",
    "ula_step_size = .001\n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "rng_seq = hk.PRNGSequence(0)\n",
    "\n",
    "means = jax.random.normal(next(rng_seq), (n_mode, dim))\n",
    "comp_dists = distrax.MultivariateNormalDiag(means, jnp.ones_like(means) * std)\n",
    "pi = distrax.Categorical(logits=jnp.zeros((n_mode,)))\n",
    "target_dist = distrax.MixtureSameFamily(pi, comp_dists)\n",
    "initial_dist = distrax.MultivariateNormalDiag(means[0] * 0 + init_mu, init_std * jnp.ones_like(means[0]))\n",
    "\n",
    "\n",
    "x_init = initial_dist.sample(seed=next(rng_seq), sample_shape=(batch_size,))\n",
    "logZs = []\n",
    "n_stepss = [100]\n",
    "\n",
    "def gradient_function(x, t):\n",
    "  t = n_steps - jnp.ones((x.shape[0],), dtype=jnp.int32) * t - 1\n",
    "  return -1 *dual_product_gradient_fn(params, next(rng_seq), x, t)\n",
    "\n",
    "if ebm:\n",
    "  def energy_function(x, t):\n",
    "    t = n_steps - jnp.ones((x.shape[0],), dtype=jnp.int32) * t - 1\n",
    "    return -1 *dual_product_energy_fn(params, next(rng_seq), x, t)\n",
    "\n",
    "for n_steps in n_stepss:\n",
    "  ula_step_sizes = jnp.ones((n_steps,)) * ula_step_size\n",
    "  uha_step_sizes = jnp.ones((n_steps,)) * uha_step_size\n",
    "\n",
    "  betas = jnp.linspace(0., 1., n_steps)\n",
    "\n",
    "  def target_function(x, i):\n",
    "    beta = betas[i]\n",
    "    init_lp = initial_dist.log_prob(x)\n",
    "    targ_lp = target_dist.log_prob(x)\n",
    "    return beta * targ_lp + (1 - beta) * init_lp\n",
    "\n",
    "  # Choose MCMC Sampler to use\n",
    "  # sampler = AnnealedULASampler(n_steps, samples_per_step, ula_step_sizes, initial_dist, target_distribution=None, gradient_function=gradient_function)\n",
    "  # sampler = AnnealedMALASampler(n_steps, samples_per_step, ula_step_sizes, initial_dist, target_distribution=None, gradient_function=gradient_function, energy_function=energy_function)\n",
    "  # sampler = AnnealedUHASampler(n_steps, samples_per_step, uha_step_sizes, damping, mass_diag_sqrt, num_leapfrog, initial_dist, target_distribution=None, gradient_function=gradient_function)\n",
    "  sampler = AnnealedMUHASampler(n_steps, samples_per_step, uha_step_sizes, damping, mass_diag_sqrt, num_leapfrog, initial_dist, target_distribution=None, gradient_function=gradient_function, energy_function=energy_function)\n",
    "\n",
    "  x_samp, logw, accept = sampler.sample(next(rng_seq), batch_size)\n",
    "  rng_seq = hk.PRNGSequence(1)\n",
    "  grad_sample = dual_product_sample_fn(params, next(rng_seq), batch_size, jnp.inf)\n",
    "\n",
    "  # Samples from MCMC\n",
    "  plt.scatter(x_samp[:, 0], x_samp[:, 1], color='green', alpha=.5)\n",
    "\n",
    "  # Samples from adding score functions\n",
    "  plt.scatter(grad_sample[:, 0], grad_sample[:, 1], color='blue', alpha=.5)\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rGBWDZVzTIa"
   },
   "source": [
    "Excute Disjunction Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "497d9d5e"
   },
   "outputs": [],
   "source": [
    "# Train Left Bar EBM Model\n",
    "batch_size = 1000\n",
    "data_dim = 2\n",
    "num_steps = 15001\n",
    "\n",
    "EMA = .999\n",
    "\n",
    "n_steps = 100\n",
    "net_params = {\"n_layers\": 4,\n",
    "              \"h_dim\": 128,\n",
    "              \"emb_dim\": 32}\n",
    "\n",
    "def forward_fn():\n",
    "  net = ResnetDiffusionModel(n_steps=n_steps, n_layers=4, x_dim=data_dim, h_dim=128, emb_dim=32)\n",
    "\n",
    "  if ebm:\n",
    "    net = EBMDiffusionModel(net)\n",
    "\n",
    "  ddpm = PortableDiffusionModel(data_dim, n_steps, net, var_type=\"beta_forward\")\n",
    "\n",
    "  def logp_unnorm(x, t):\n",
    "    scale_e = ddpm.energy_scale(-2 - t)\n",
    "    t = jnp.ones((x.shape[0],), dtype=jnp.int32) * t\n",
    "    return -net.neg_logp_unnorm(x, t) * scale_e\n",
    "\n",
    "  def _logpx(x):\n",
    "    return ddpm.logpx(x)[\"logpx\"]\n",
    "  return ddpm.loss, (ddpm.loss, ddpm.sample, _logpx, logp_unnorm)\n",
    "\n",
    "forward = hk.multi_transform(forward_fn)\n",
    "rng_seq = hk.PRNGSequence(0)\n",
    "\n",
    "xr = [-.75, .75]\n",
    "yr = [-.75, .75]\n",
    "\n",
    "\n",
    "def plot_samples(x):\n",
    "  plt.scatter(x[:, 0], x[:, 1])\n",
    "  plt.xlim(-2, 2)\n",
    "  plt.ylim(-2, 2)\n",
    "\n",
    "def dist_show_2d(fn, xr, yr):\n",
    "    nticks = 100\n",
    "    x, y = np.meshgrid(np.linspace(xr[0], xr[1], nticks), np.linspace(yr[0], yr[1], nticks))\n",
    "    coord = np.stack([x, y], axis=-1).reshape((-1, 2))\n",
    "    heatmap = fn(coord).reshape((nticks, nticks))\n",
    "    plt.imshow(heatmap)\n",
    "\n",
    "# load data\n",
    "dataset_energy, dataset_sample = left_bar()\n",
    "x = dataset_sample(batch_size)\n",
    "\n",
    "plot_samples(x)\n",
    "plt.show()\n",
    "x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "\n",
    "params = forward.init(next(rng_seq), x)\n",
    "loss_fn, sample_fn, logpx_fn, logp_unnorm_fn = forward.apply\n",
    "param_count = sum(x.size for x in jax.tree_leaves(params))\n",
    "for k, v in jax.tree_map(lambda x: x.shape, params).items():\n",
    "  print(k, v)\n",
    "print(\"Model has {} params\".format(param_count))\n",
    "\n",
    "\n",
    "opt = optax.adam(1e-3)\n",
    "opt_state = opt.init(params)\n",
    "\n",
    "sample_fn = jax.jit(sample_fn, static_argnums=2)\n",
    "logpx_fn = jax.jit(logpx_fn)\n",
    "\n",
    "logp_unnorm_fn = jax.jit(logp_unnorm_fn)\n",
    "\n",
    "@jax.jit\n",
    "def mean_loss_fn(params, rng, x):\n",
    "  loss = loss_fn(params, rng, x)\n",
    "  return loss.mean()\n",
    "\n",
    "@jax.jit\n",
    "def update(params, opt_state, rng, x):\n",
    "  loss, grad = jax.value_and_grad(mean_loss_fn)(params, rng, x)\n",
    "\n",
    "  updates, opt_state = opt.update(grad, opt_state)\n",
    "  new_params = optax.apply_updates(params, updates)\n",
    "  return loss, new_params, opt_state\n",
    "\n",
    "ema_params = params\n",
    "losses = []\n",
    "test_logpx = []\n",
    "itr = 0\n",
    "\n",
    "for itr in range(num_steps):\n",
    "  x = dataset_sample(batch_size)\n",
    "\n",
    "  x = x.reshape(x.shape[0], -1)\n",
    "  start_time = time.time()\n",
    "  loss, params, opt_state = update(params, opt_state, next(rng_seq), x)\n",
    "  duration_update = time.time() - start_time\n",
    "  ema_params = jax.tree_map(lambda e, p: e * EMA + p * (1 - EMA), ema_params, params)\n",
    "\n",
    "  if itr % 100 == 0:\n",
    "    print(itr, loss, \"time:\", duration_update)\n",
    "    losses.append(loss)\n",
    "  if itr % 1000 == 0:\n",
    "    x_samp = sample_fn(ema_params, next(rng_seq), batch_size)\n",
    "    plot_samples(x_samp)\n",
    "    plt.show()\n",
    "    logpx = logpx_fn(ema_params, next(rng_seq), x).mean()\n",
    "    print(\"TEST\", itr, 'logpx', logpx)\n",
    "    test_logpx.append(logpx)\n",
    "\n",
    "    if ebm:\n",
    "      for t in range(10):\n",
    "        dist_show_2d(lambda x: logp_unnorm_fn(ema_params, next(rng_seq), x, 10*t), xr=xr, yr=yr)\n",
    "        plt.show()\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "plt.plot(test_logpx)\n",
    "\n",
    "flow_params = params\n",
    "flow_rng = next(rng_seq)\n",
    "\n",
    "left_bar_params = ema_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58055777"
   },
   "outputs": [],
   "source": [
    "# Train Right Bar EBM Model\n",
    "batch_size = 1000\n",
    "data_dim = 2\n",
    "num_steps = 15001\n",
    "\n",
    "EMA = .999\n",
    "\n",
    "n_steps = 100\n",
    "net_params = {\"n_layers\": 4,\n",
    "              \"h_dim\": 128,\n",
    "              \"emb_dim\": 32}\n",
    "\n",
    "def forward_fn():\n",
    "  net = ResnetDiffusionModel(n_steps=n_steps, n_layers=4, x_dim=data_dim, h_dim=128, emb_dim=32)\n",
    "\n",
    "  if ebm:\n",
    "    net = EBMDiffusionModel(net)\n",
    "\n",
    "  ddpm = PortableDiffusionModel(data_dim, n_steps, net, var_type=\"beta_forward\")\n",
    "\n",
    "  def logp_unnorm(x, t):\n",
    "    scale_e = ddpm.energy_scale(-2 - t)\n",
    "    t = jnp.ones((x.shape[0],), dtype=jnp.int32) * t\n",
    "    return -net.neg_logp_unnorm(x, t) * scale_e\n",
    "\n",
    "  def _logpx(x):\n",
    "    return ddpm.logpx(x)[\"logpx\"]\n",
    "  return ddpm.loss, (ddpm.loss, ddpm.sample, _logpx, logp_unnorm)\n",
    "\n",
    "forward = hk.multi_transform(forward_fn)\n",
    "rng_seq = hk.PRNGSequence(0)\n",
    "\n",
    "xr = [-.75, .75]\n",
    "yr = [-.75, .75]\n",
    "\n",
    "\n",
    "def plot_samples(x):\n",
    "  plt.scatter(x[:, 0], x[:, 1])\n",
    "  plt.xlim(-2, 2)\n",
    "  plt.ylim(-2, 2)\n",
    "\n",
    "def dist_show_2d(fn, xr, yr):\n",
    "    nticks = 100\n",
    "    x, y = np.meshgrid(np.linspace(xr[0], xr[1], nticks), np.linspace(yr[0], yr[1], nticks))\n",
    "    coord = np.stack([x, y], axis=-1).reshape((-1, 2))\n",
    "    heatmap = fn(coord).reshape((nticks, nticks))\n",
    "    plt.imshow(heatmap)\n",
    "\n",
    "# load data\n",
    "dataset_energy, dataset_sample = right_bar()\n",
    "x = dataset_sample(batch_size)\n",
    "\n",
    "plot_samples(x)\n",
    "plt.show()\n",
    "x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "\n",
    "params = forward.init(next(rng_seq), x)\n",
    "loss_fn, sample_fn, logpx_fn, logp_unnorm_fn = forward.apply\n",
    "param_count = sum(x.size for x in jax.tree_leaves(params))\n",
    "for k, v in jax.tree_map(lambda x: x.shape, params).items():\n",
    "  print(k, v)\n",
    "print(\"Model has {} params\".format(param_count))\n",
    "\n",
    "\n",
    "opt = optax.adam(1e-3)\n",
    "opt_state = opt.init(params)\n",
    "\n",
    "sample_fn = jax.jit(sample_fn, static_argnums=2)\n",
    "logpx_fn = jax.jit(logpx_fn)\n",
    "\n",
    "logp_unnorm_fn = jax.jit(logp_unnorm_fn)\n",
    "\n",
    "@jax.jit\n",
    "def mean_loss_fn(params, rng, x):\n",
    "  loss = loss_fn(params, rng, x)\n",
    "  return loss.mean()\n",
    "\n",
    "@jax.jit\n",
    "def update(params, opt_state, rng, x):\n",
    "  loss, grad = jax.value_and_grad(mean_loss_fn)(params, rng, x)\n",
    "\n",
    "  updates, opt_state = opt.update(grad, opt_state)\n",
    "  new_params = optax.apply_updates(params, updates)\n",
    "  return loss, new_params, opt_state\n",
    "\n",
    "ema_params = params\n",
    "losses = []\n",
    "test_logpx = []\n",
    "itr = 0\n",
    "\n",
    "for itr in range(num_steps):\n",
    "  x = dataset_sample(batch_size)\n",
    "\n",
    "  x = x.reshape(x.shape[0], -1)\n",
    "  start_time = time.time()\n",
    "  loss, params, opt_state = update(params, opt_state, next(rng_seq), x)\n",
    "  duration_update = time.time() - start_time\n",
    "  ema_params = jax.tree_map(lambda e, p: e * EMA + p * (1 - EMA), ema_params, params)\n",
    "\n",
    "  if itr % 100 == 0:\n",
    "    print(itr, loss, \"time:\", duration_update)\n",
    "    losses.append(loss)\n",
    "  if itr % 1000 == 0:\n",
    "    x_samp = sample_fn(ema_params, next(rng_seq), batch_size)\n",
    "    plot_samples(x_samp)\n",
    "    plt.show()\n",
    "    logpx = logpx_fn(ema_params, next(rng_seq), x).mean()\n",
    "    print(\"TEST\", itr, 'logpx', logpx)\n",
    "    test_logpx.append(logpx)\n",
    "\n",
    "    if ebm:\n",
    "      for t in range(10):\n",
    "        dist_show_2d(lambda x: logp_unnorm_fn(ema_params, next(rng_seq), x, 10*t), xr=xr, yr=yr)\n",
    "        plt.show()\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "plt.plot(test_logpx)\n",
    "\n",
    "flow_params = params\n",
    "flow_rng = next(rng_seq)\n",
    "\n",
    "right_bar_params = ema_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6b61c646"
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "\n",
    "for k, v in left_bar_params.items():\n",
    "  params[k] = v\n",
    "\n",
    "for k, v in right_bar_params.items():\n",
    "  k = k.replace('resnet_diffusion_model/', 'resnet_diffusion_model_1/')\n",
    "  params[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "50d8d543",
    "outputId": "309fe827-ad1a-427c-d129-ba214f627b34"
   },
   "outputs": [],
   "source": [
    "# Sampling from disjunction of distributions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dim = 2\n",
    "n_mode = 4\n",
    "std = .05\n",
    "init_std = 1.\n",
    "init_mu = 0.\n",
    "n_steps = 10\n",
    "damping = .5\n",
    "mass_diag_sqrt = 1.\n",
    "num_leapfrog = 3\n",
    "samples_per_step = 10\n",
    "uha_step_size = .03\n",
    "ula_step_size = .001\n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "rng_seq = hk.PRNGSequence(0)\n",
    "\n",
    "means = jax.random.normal(next(rng_seq), (n_mode, dim))\n",
    "comp_dists = distrax.MultivariateNormalDiag(means, jnp.ones_like(means) * std)\n",
    "pi = distrax.Categorical(logits=jnp.zeros((n_mode,)))\n",
    "target_dist = distrax.MixtureSameFamily(pi, comp_dists)\n",
    "initial_dist = distrax.MultivariateNormalDiag(means[0] * 0 + init_mu, init_std * jnp.ones_like(means[0]))\n",
    "\n",
    "\n",
    "x_init = initial_dist.sample(seed=next(rng_seq), sample_shape=(batch_size,))\n",
    "logZs = []\n",
    "n_stepss = [100]\n",
    "\n",
    "def gradient_function(x, t):\n",
    "  t = n_steps - jnp.ones((x.shape[0],), dtype=jnp.int32) * t - 1\n",
    "  return -1 *dual_mixture_gradient_fn(params, next(rng_seq), x, t)\n",
    "\n",
    "if ebm:\n",
    "  def energy_function(x, t):\n",
    "    t = n_steps - jnp.ones((x.shape[0],), dtype=jnp.int32) * t - 1\n",
    "    return -1 *dual_mixture_energy_fn(params, next(rng_seq), x, t)\n",
    "\n",
    "for n_steps in n_stepss:\n",
    "  ula_step_sizes = jnp.ones((n_steps,)) * ula_step_size\n",
    "  uha_step_sizes = jnp.ones((n_steps,)) * uha_step_size\n",
    "\n",
    "  betas = jnp.linspace(0., 1., n_steps)\n",
    "\n",
    "  def target_function(x, i):\n",
    "    beta = betas[i]\n",
    "    init_lp = initial_dist.log_prob(x)\n",
    "    targ_lp = target_dist.log_prob(x)\n",
    "    return beta * targ_lp + (1 - beta) * init_lp\n",
    "\n",
    "  # Choose the sampler to use\n",
    "  # sampler = AnnealedULASampler(n_steps, samples_per_step, ula_step_sizes, initial_dist, target_distribution=None, gradient_function=gradient_function)\n",
    "  # sampler = AnnealedMALASampler(n_steps, samples_per_step, ula_step_sizes, initial_dist, target_distribution=None, gradient_function=gradient_function, energy_function=energy_function)\n",
    "  # sampler = AnnealedUHASampler(n_steps, samples_per_step, uha_step_sizes, damping, mass_diag_sqrt, num_leapfrog, initial_dist, target_distribution=None, gradient_function=gradient_function)\n",
    "  sampler = AnnealedMUHASampler(n_steps, samples_per_step, uha_step_sizes, damping, mass_diag_sqrt, num_leapfrog, initial_dist, target_distribution=None, gradient_function=gradient_function, energy_function=energy_function)\n",
    "\n",
    "  x_samp, logw, accept = sampler.sample(next(rng_seq), batch_size)\n",
    "  rng_seq = hk.PRNGSequence(1)\n",
    "  grad_sample = dual_mixture_sample_fn(params, next(rng_seq), batch_size, jnp.inf)\n",
    "\n",
    "  # Samples from MCMC\n",
    "  plt.scatter(x_samp[:, 0], x_samp[:, 1], color='green', alpha=.5)\n",
    "\n",
    "  # Samples generated from adding score function\n",
    "  plt.scatter(grad_sample[:, 0], grad_sample[:, 1], color='blue', alpha=.5)\n",
    "\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXJKzux3zMjf"
   },
   "source": [
    "Excute Negation Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0feb4053"
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "data_dim = 2\n",
    "num_steps = 15001\n",
    "\n",
    "EMA = .999\n",
    "\n",
    "n_steps = 100\n",
    "net_params = {\"n_layers\": 4,\n",
    "              \"h_dim\": 128,\n",
    "              \"emb_dim\": 32}\n",
    "\n",
    "def forward_fn():\n",
    "  net = ResnetDiffusionModel(n_steps=n_steps, n_layers=4, x_dim=data_dim, h_dim=128, emb_dim=32)\n",
    "\n",
    "  if ebm:\n",
    "    net = EBMDiffusionModel(net)\n",
    "\n",
    "  ddpm = PortableDiffusionModel(data_dim, n_steps, net, var_type=\"beta_forward\")\n",
    "\n",
    "  def logp_unnorm(x, t):\n",
    "    scale_e = ddpm.energy_scale(-2 - t)\n",
    "    t = jnp.ones((x.shape[0],), dtype=jnp.int32) * t\n",
    "    return -net.neg_logp_unnorm(x, t) * scale_e\n",
    "\n",
    "  def _logpx(x):\n",
    "    return ddpm.logpx(x)[\"logpx\"]\n",
    "  return ddpm.loss, (ddpm.loss, ddpm.sample, _logpx, logp_unnorm)\n",
    "\n",
    "forward = hk.multi_transform(forward_fn)\n",
    "rng_seq = hk.PRNGSequence(0)\n",
    "\n",
    "xr = [-.75, .75]\n",
    "yr = [-.75, .75]\n",
    "\n",
    "\n",
    "def plot_samples(x):\n",
    "  plt.scatter(x[:, 0], x[:, 1])\n",
    "  plt.xlim(-2, 2)\n",
    "  plt.ylim(-2, 2)\n",
    "\n",
    "def dist_show_2d(fn, xr, yr):\n",
    "    nticks = 100\n",
    "    x, y = np.meshgrid(np.linspace(xr[0], xr[1], nticks), np.linspace(yr[0], yr[1], nticks))\n",
    "    coord = np.stack([x, y], axis=-1).reshape((-1, 2))\n",
    "    heatmap = fn(coord).reshape((nticks, nticks))\n",
    "    plt.imshow(heatmap)\n",
    "\n",
    "# load data\n",
    "dataset_energy, dataset_sample = toy_box(scale=0.7)\n",
    "x = dataset_sample(batch_size)\n",
    "\n",
    "plot_samples(x)\n",
    "plt.show()\n",
    "x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "\n",
    "params = forward.init(next(rng_seq), x)\n",
    "loss_fn, sample_fn, logpx_fn, logp_unnorm_fn = forward.apply\n",
    "param_count = sum(x.size for x in jax.tree_leaves(params))\n",
    "for k, v in jax.tree_map(lambda x: x.shape, params).items():\n",
    "  print(k, v)\n",
    "print(\"Model has {} params\".format(param_count))\n",
    "\n",
    "\n",
    "opt = optax.adam(1e-3)\n",
    "opt_state = opt.init(params)\n",
    "\n",
    "sample_fn = jax.jit(sample_fn, static_argnums=2)\n",
    "logpx_fn = jax.jit(logpx_fn)\n",
    "\n",
    "logp_unnorm_fn = jax.jit(logp_unnorm_fn)\n",
    "\n",
    "@jax.jit\n",
    "def mean_loss_fn(params, rng, x):\n",
    "  loss = loss_fn(params, rng, x)\n",
    "  return loss.mean()\n",
    "\n",
    "@jax.jit\n",
    "def update(params, opt_state, rng, x):\n",
    "  loss, grad = jax.value_and_grad(mean_loss_fn)(params, rng, x)\n",
    "\n",
    "  updates, opt_state = opt.update(grad, opt_state)\n",
    "  new_params = optax.apply_updates(params, updates)\n",
    "  return loss, new_params, opt_state\n",
    "\n",
    "ema_params = params\n",
    "losses = []\n",
    "test_logpx = []\n",
    "itr = 0\n",
    "\n",
    "for itr in range(num_steps):\n",
    "  x = dataset_sample(batch_size)\n",
    "\n",
    "  x = x.reshape(x.shape[0], -1)\n",
    "  start_time = time.time()\n",
    "  loss, params, opt_state = update(params, opt_state, next(rng_seq), x)\n",
    "  duration_update = time.time() - start_time\n",
    "  ema_params = jax.tree_map(lambda e, p: e * EMA + p * (1 - EMA), ema_params, params)\n",
    "\n",
    "  if itr % 100 == 0:\n",
    "    print(itr, loss, \"time:\", duration_update)\n",
    "    losses.append(loss)\n",
    "  if itr % 1000 == 0:\n",
    "    x_samp = sample_fn(ema_params, next(rng_seq), batch_size)\n",
    "    plot_samples(x_samp)\n",
    "    plt.show()\n",
    "    logpx = logpx_fn(ema_params, next(rng_seq), x).mean()\n",
    "    print(\"TEST\", itr, 'logpx', logpx)\n",
    "    test_logpx.append(logpx)\n",
    "\n",
    "    if ebm:\n",
    "      for t in range(10):\n",
    "        dist_show_2d(lambda x: logp_unnorm_fn(ema_params, next(rng_seq), x, 10*t), xr=xr, yr=yr)\n",
    "        plt.show()\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "plt.plot(test_logpx)\n",
    "\n",
    "large_box_params = ema_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "781d60a9"
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "data_dim = 2\n",
    "num_steps = 15001\n",
    "\n",
    "EMA = .999\n",
    "\n",
    "n_steps = 100\n",
    "net_params = {\"n_layers\": 4,\n",
    "              \"h_dim\": 128,\n",
    "              \"emb_dim\": 32}\n",
    "\n",
    "def forward_fn():\n",
    "  net = ResnetDiffusionModel(n_steps=n_steps, n_layers=4, x_dim=data_dim, h_dim=128, emb_dim=32)\n",
    "\n",
    "  if ebm:\n",
    "    net = EBMDiffusionModel(net)\n",
    "\n",
    "  ddpm = PortableDiffusionModel(data_dim, n_steps, net, var_type=\"beta_forward\")\n",
    "\n",
    "  def logp_unnorm(x, t):\n",
    "    scale_e = ddpm.energy_scale(-2 - t)\n",
    "    t = jnp.ones((x.shape[0],), dtype=jnp.int32) * t\n",
    "    return -net.neg_logp_unnorm(x, t) * scale_e\n",
    "\n",
    "  def _logpx(x):\n",
    "    return ddpm.logpx(x)[\"logpx\"]\n",
    "  return ddpm.loss, (ddpm.loss, ddpm.sample, _logpx, logp_unnorm)\n",
    "\n",
    "forward = hk.multi_transform(forward_fn)\n",
    "rng_seq = hk.PRNGSequence(0)\n",
    "\n",
    "xr = [-.75, .75]\n",
    "yr = [-.75, .75]\n",
    "\n",
    "\n",
    "def plot_samples(x):\n",
    "  plt.scatter(x[:, 0], x[:, 1])\n",
    "  plt.xlim(-2, 2)\n",
    "  plt.ylim(-2, 2)\n",
    "\n",
    "def dist_show_2d(fn, xr, yr):\n",
    "    nticks = 100\n",
    "    x, y = np.meshgrid(np.linspace(xr[0], xr[1], nticks), np.linspace(yr[0], yr[1], nticks))\n",
    "    coord = np.stack([x, y], axis=-1).reshape((-1, 2))\n",
    "    heatmap = fn(coord).reshape((nticks, nticks))\n",
    "    plt.imshow(heatmap)\n",
    "\n",
    "# load data\n",
    "dataset_energy, dataset_sample = toy_box(scale=0.3)\n",
    "x = dataset_sample(batch_size)\n",
    "\n",
    "plot_samples(x)\n",
    "plt.show()\n",
    "x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "\n",
    "params = forward.init(next(rng_seq), x)\n",
    "loss_fn, sample_fn, logpx_fn, logp_unnorm_fn = forward.apply\n",
    "param_count = sum(x.size for x in jax.tree_leaves(params))\n",
    "for k, v in jax.tree_map(lambda x: x.shape, params).items():\n",
    "  print(k, v)\n",
    "print(\"Model has {} params\".format(param_count))\n",
    "\n",
    "\n",
    "opt = optax.adam(1e-3)\n",
    "opt_state = opt.init(params)\n",
    "\n",
    "sample_fn = jax.jit(sample_fn, static_argnums=2)\n",
    "logpx_fn = jax.jit(logpx_fn)\n",
    "\n",
    "logp_unnorm_fn = jax.jit(logp_unnorm_fn)\n",
    "\n",
    "@jax.jit\n",
    "def mean_loss_fn(params, rng, x):\n",
    "  loss = loss_fn(params, rng, x)\n",
    "  return loss.mean()\n",
    "\n",
    "@jax.jit\n",
    "def update(params, opt_state, rng, x):\n",
    "  loss, grad = jax.value_and_grad(mean_loss_fn)(params, rng, x)\n",
    "\n",
    "  updates, opt_state = opt.update(grad, opt_state)\n",
    "  new_params = optax.apply_updates(params, updates)\n",
    "  return loss, new_params, opt_state\n",
    "\n",
    "ema_params = params\n",
    "losses = []\n",
    "test_logpx = []\n",
    "itr = 0\n",
    "\n",
    "for itr in range(num_steps):\n",
    "  x = dataset_sample(batch_size)\n",
    "\n",
    "  x = x.reshape(x.shape[0], -1)\n",
    "  start_time = time.time()\n",
    "  loss, params, opt_state = update(params, opt_state, next(rng_seq), x)\n",
    "  duration_update = time.time() - start_time\n",
    "  ema_params = jax.tree_map(lambda e, p: e * EMA + p * (1 - EMA), ema_params, params)\n",
    "\n",
    "  if itr % 100 == 0:\n",
    "    print(itr, loss, \"time:\", duration_update)\n",
    "    losses.append(loss)\n",
    "  if itr % 1000 == 0:\n",
    "    x_samp = sample_fn(ema_params, next(rng_seq), batch_size)\n",
    "    plot_samples(x_samp)\n",
    "    plt.show()\n",
    "    logpx = logpx_fn(ema_params, next(rng_seq), x).mean()\n",
    "    print(\"TEST\", itr, 'logpx', logpx)\n",
    "    test_logpx.append(logpx)\n",
    "\n",
    "    if ebm:\n",
    "      for t in range(10):\n",
    "        dist_show_2d(lambda x: logp_unnorm_fn(ema_params, next(rng_seq), x, 10*t), xr=xr, yr=yr)\n",
    "        plt.show()\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "plt.plot(test_logpx)\n",
    "\n",
    "small_box_params = ema_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G3W6TLbqy3Xs"
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "\n",
    "for k, v in large_box_params.items():\n",
    "  params[k] = v\n",
    "\n",
    "for k, v in small_box_params.items():\n",
    "  k = k.replace('resnet_diffusion_model/', 'resnet_diffusion_model_1/')\n",
    "  params[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "277d91bf",
    "outputId": "aaf6e302-e3df-4483-dfb6-33483a5b24bb"
   },
   "outputs": [],
   "source": [
    "# Sampling from negation of distributions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dim = 2\n",
    "n_mode = 4\n",
    "std = .05\n",
    "init_std = 1.\n",
    "init_mu = 0.\n",
    "n_steps = 10\n",
    "damping = .5\n",
    "mass_diag_sqrt = 1.\n",
    "num_leapfrog = 3\n",
    "samples_per_step = 10\n",
    "uha_step_size = .03\n",
    "ula_step_size = .001\n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "rng_seq = hk.PRNGSequence(0)\n",
    "\n",
    "means = jax.random.normal(next(rng_seq), (n_mode, dim))\n",
    "comp_dists = distrax.MultivariateNormalDiag(means, jnp.ones_like(means) * std)\n",
    "pi = distrax.Categorical(logits=jnp.zeros((n_mode,)))\n",
    "target_dist = distrax.MixtureSameFamily(pi, comp_dists)\n",
    "initial_dist = distrax.MultivariateNormalDiag(means[0] * 0 + init_mu, init_std * jnp.ones_like(means[0]))\n",
    "\n",
    "\n",
    "x_init = initial_dist.sample(seed=next(rng_seq), sample_shape=(batch_size,))\n",
    "logZs = []\n",
    "n_stepss = [100]\n",
    "\n",
    "def gradient_function(x, t):\n",
    "  t = n_steps - jnp.ones((x.shape[0],), dtype=jnp.int32) * t - 1\n",
    "  return -1 *dual_negation_gradient_fn(params, next(rng_seq), x, t)\n",
    "\n",
    "if ebm:\n",
    "  def energy_function(x, t):\n",
    "    t = n_steps - jnp.ones((x.shape[0],), dtype=jnp.int32) * t - 1\n",
    "    return -1 *dual_negation_energy_fn(params, next(rng_seq), x, t)\n",
    "\n",
    "for n_steps in n_stepss:\n",
    "  ula_step_sizes = jnp.ones((n_steps,)) * ula_step_size\n",
    "  uha_step_sizes = jnp.ones((n_steps,)) * uha_step_size\n",
    "\n",
    "  betas = jnp.linspace(0., 1., n_steps)\n",
    "\n",
    "  def target_function(x, i):\n",
    "    #print(x.shape)\n",
    "    beta = betas[i]\n",
    "    init_lp = initial_dist.log_prob(x)\n",
    "    targ_lp = target_dist.log_prob(x)\n",
    "    return beta * targ_lp + (1 - beta) * init_lp\n",
    "\n",
    "  # Choose the sampler to use\n",
    "  # sampler = AnnealedULASampler(n_steps, samples_per_step, ula_step_sizes, initial_dist, target_distribution=None, gradient_function=gradient_function)\n",
    "  # sampler = AnnealedMALASampler(n_steps, samples_per_step, ula_step_sizes, initial_dist, target_distribution=None, gradient_function=gradient_function, energy_function=energy_function)\n",
    "  # sampler = AnnealedUHASampler(n_steps, samples_per_step, uha_step_sizes, damping, mass_diag_sqrt, num_leapfrog, initial_dist, target_distribution=None, gradient_function=gradient_function)\n",
    "  sampler = AnnealedMUHASampler(n_steps, samples_per_step, uha_step_sizes, damping, mass_diag_sqrt, num_leapfrog, initial_dist, target_distribution=None, gradient_function=gradient_function, energy_function=energy_function)\n",
    "\n",
    "  # Generate sample with MCMC\n",
    "  x_samp, logw, accept = sampler.sample(next(rng_seq), batch_size)\n",
    "  rng_seq = hk.PRNGSequence(1)\n",
    "\n",
    "  # Samples generated by adding score function\n",
    "  grad_sample = dual_negation_sample_fn(params, next(rng_seq), batch_size, jnp.inf)\n",
    "\n",
    "  # Samples from MCMC\n",
    "  plt.scatter(x_samp[:, 0], x_samp[:, 1], color='green', alpha=.5)\n",
    "\n",
    "  # Samples from adding score\n",
    "  plt.scatter(grad_sample[:, 0], grad_sample[:, 1], color='blue', alpha=.5)\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf11afa1"
   },
   "outputs": [],
   "source": [
    "def gen_plot_nice(pts):\n",
    "    plt.scatter(pts[:, 0], pts[:, 1], c='b')\n",
    "    ax = plt.gca()\n",
    "\n",
    "    plt.xlim(-1, 1)\n",
    "    plt.ylim(-1, 1)\n",
    "\n",
    "    for axis in ['top', 'bottom', 'left', 'right']:\n",
    "        ax.spines[axis].set_linewidth(1.5)\n",
    "\n",
    "    ax.set_xticks([], [])\n",
    "    ax.set_yticks([], [])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "ebm_diff_score",
   "language": "python",
   "name": "ebm_diff_score"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
